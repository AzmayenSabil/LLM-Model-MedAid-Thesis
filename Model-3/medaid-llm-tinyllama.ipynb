{"cells":[{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T16:47:11.997733Z","iopub.status.busy":"2024-01-27T16:47:11.996831Z","iopub.status.idle":"2024-01-27T16:47:12.001670Z","shell.execute_reply":"2024-01-27T16:47:12.000703Z","shell.execute_reply.started":"2024-01-27T16:47:11.997700Z"},"trusted":true},"outputs":[],"source":["# import os\n","\n","# # List files in the input directory\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T16:47:12.003743Z","iopub.status.busy":"2024-01-27T16:47:12.003397Z","iopub.status.idle":"2024-01-27T16:47:12.014611Z","shell.execute_reply":"2024-01-27T16:47:12.013789Z","shell.execute_reply.started":"2024-01-27T16:47:12.003718Z"},"trusted":true},"outputs":[],"source":["# !pwd\n","# import sys\n","# sys.path.append('/kaggle/working/packages')  # Replace with the actual path\n","# !echo $PYTHONPATH"]},{"cell_type":"markdown","metadata":{},"source":["**To delete a specific folder of kaggle**"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T16:47:12.016014Z","iopub.status.busy":"2024-01-27T16:47:12.015748Z","iopub.status.idle":"2024-01-27T16:47:12.024992Z","shell.execute_reply":"2024-01-27T16:47:12.024114Z","shell.execute_reply.started":"2024-01-27T16:47:12.015992Z"},"trusted":true},"outputs":[],"source":["# # Clear output folder\n","# import os\n","\n","# def remove_folder_contents(folder):\n","#     for the_file in os.listdir(folder):\n","#         file_path = os.path.join(folder, the_file)\n","#         try:\n","#             if os.path.isfile(file_path):\n","#                 os.unlink(file_path)\n","#             elif os.path.isdir(file_path):\n","#                 remove_folder_contents(file_path)\n","#                 os.rmdir(file_path)\n","#         except Exception as e:\n","#             print(e)\n","\n","# folder_path = '/kaggle/working/packages'\n","# remove_folder_contents(folder_path)\n","# os.rmdir(folder_path)"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T16:47:12.027287Z","iopub.status.busy":"2024-01-27T16:47:12.026981Z","iopub.status.idle":"2024-01-27T16:47:42.426119Z","shell.execute_reply":"2024-01-27T16:47:42.425125Z","shell.execute_reply.started":"2024-01-27T16:47:12.027264Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: accelerate in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (0.26.1)Note: you may need to restart the kernel to use updated packages.\n","\n","Requirement already satisfied: peft in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (0.8.2)\n","Requirement already satisfied: transformers in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (4.37.2)\n","Requirement already satisfied: pdfplumber in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (0.10.3)\n","Requirement already satisfied: datasets==2.16.1 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (2.16.1)\n","Requirement already satisfied: trl in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (0.7.10)\n","Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from datasets==2.16.1) (3.13.1)\n","Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from datasets==2.16.1) (1.24.4)\n","Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from datasets==2.16.1) (15.0.0)\n","Requirement already satisfied: pyarrow-hotfix in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from datasets==2.16.1) (0.6)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from datasets==2.16.1) (0.3.7)\n","Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from datasets==2.16.1) (2.0.3)\n","Requirement already satisfied: requests>=2.19.0 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from datasets==2.16.1) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from datasets==2.16.1) (4.66.1)\n","Requirement already satisfied: xxhash in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from datasets==2.16.1) (3.4.1)\n","Requirement already satisfied: multiprocess in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from datasets==2.16.1) (0.70.15)\n","Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.16.1) (2023.10.0)\n","Requirement already satisfied: aiohttp in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from datasets==2.16.1) (3.9.1)\n","Requirement already satisfied: huggingface-hub>=0.19.4 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from datasets==2.16.1) (0.20.3)\n","Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from datasets==2.16.1) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from datasets==2.16.1) (6.0.1)\n","Requirement already satisfied: psutil in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from accelerate) (5.9.8)\n","Requirement already satisfied: torch>=1.10.0 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from accelerate) (2.1.2)\n","Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from accelerate) (0.4.2)\n","Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from transformers) (0.15.1)\n","Requirement already satisfied: pdfminer.six==20221105 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from pdfplumber) (20221105)\n","Requirement already satisfied: Pillow>=9.1 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from pdfplumber) (10.2.0)\n","Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from pdfplumber) (4.26.0)\n","Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from pdfminer.six==20221105->pdfplumber) (3.3.2)\n","Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from pdfminer.six==20221105->pdfplumber) (42.0.1)\n","Requirement already satisfied: tyro>=0.5.11 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from trl) (0.7.0)\n","Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from aiohttp->datasets==2.16.1) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from aiohttp->datasets==2.16.1) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from aiohttp->datasets==2.16.1) (1.9.4)\n","Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from aiohttp->datasets==2.16.1) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from aiohttp->datasets==2.16.1) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from aiohttp->datasets==2.16.1) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets==2.16.1) (4.9.0)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from requests>=2.19.0->datasets==2.16.1) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from requests>=2.19.0->datasets==2.16.1) (2.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from requests>=2.19.0->datasets==2.16.1) (2023.11.17)\n","Requirement already satisfied: sympy in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1)\n","Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n","Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from tqdm>=4.62.1->datasets==2.16.1) (0.4.6)\n","Requirement already satisfied: docstring-parser>=0.14.1 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from tyro>=0.5.11->trl) (0.15)\n","Requirement already satisfied: rich>=11.1.0 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from tyro>=0.5.11->trl) (13.7.0)\n","Requirement already satisfied: shtab>=1.5.6 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from tyro>=0.5.11->trl) (1.6.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from pandas->datasets==2.16.1) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from pandas->datasets==2.16.1) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.1 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from pandas->datasets==2.16.1) (2023.4)\n","Requirement already satisfied: cffi>=1.12 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (1.16.0)\n","Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.16.1) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.17.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.4)\n","Requirement already satisfied: mpmath>=0.19 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Requirement already satisfied: pycparser in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (2.21)\n","Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n","Looking in indexes: https://pypi.org/simple, https://jllllll.github.io/bitsandbytes-windows-webui\n","Requirement already satisfied: bitsandbytes in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (0.42.0)\n","Requirement already satisfied: scipy in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from bitsandbytes) (1.10.1)\n","Requirement already satisfied: numpy<1.27.0,>=1.19.5 in c:\\users\\user\\anaconda3\\envs\\tinyllamamedaid\\lib\\site-packages (from scipy->bitsandbytes) (1.24.4)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["# !pip install --upgrade accelerate peft bitsandbytes transformers pdfplumber datasets==2.16.1 trl==0.7.9 --target=/kaggle/working/packages\n","%pip install --upgrade accelerate peft transformers pdfplumber datasets==2.16.1 trl\n","%pip install bitsandbytes --prefer-binary --extra-index-url=https://jllllll.github.io/bitsandbytes-windows-webui\n"]},{"cell_type":"markdown","metadata":{},"source":["**To resolve some issues while installing packages**"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T16:47:42.428523Z","iopub.status.busy":"2024-01-27T16:47:42.427733Z","iopub.status.idle":"2024-01-27T16:47:42.434850Z","shell.execute_reply":"2024-01-27T16:47:42.433821Z","shell.execute_reply.started":"2024-01-27T16:47:42.428486Z"},"trusted":true},"outputs":[],"source":["# import pkg_resources\n","\n","# def resolve_conflicts():\n","#     \"\"\"\n","#     Resolves dependency conflicts by creating a new environment and installing compatible versions.\n","#     \"\"\"\n","\n","#     # Create a new virtual environment (replace \"myvenv\" with your desired name)\n","#     !virtualenv myvenv\n","#     !source myvenv/bin/activate\n","\n","#     # Install compatible versions of conflicting packages\n","#     required_versions = {\n","#         \"cupy-cuda11x\": \">=12.0.0\",\n","#         \"dill\": \"<0.3.2,>=0.3.1.1\",\n","#         \"numpy\": \"<1.25.0,>=1.14.3\",\n","#         \"pyarrow\": \"<10.0.0,>=3.0.0\",\n","#         \"jupyter-server\": \"~=1.16\",\n","#         \"jupyterlab\": \"~=3.4\",\n","#         \"urllib3\": \"<2.1,>=1.25.4\",  # Ensure python_version >= \"3.10\"\n","#         \"pandas\": \"<1.6.0dev0,>=1.3\",\n","#         \"protobuf\": \"<5,>=4.21\",\n","#         \"dask\": \"==2023.7.1\",\n","#         \"distributed\": \"==2023.7.1\",\n","#         \"fsspec\": \"==2023.6.0\",\n","#         \"urllib3\": \"<2.0\",\n","#         \"google-api-core[grpc]\": \"<2.0.0dev,>=1.22.2\",\n","#         \"packaging\": \"<22.0dev,>=14.3\",\n","#         \"grpcio\": \"<2.0dev,>=1.51.3\",\n","#         \"jupyter-lsp\": \">=2.0.0\",\n","#         \"google-cloud-storage\": \"<3,>=2.2.1\",\n","#         \"shapely\": \">=2.0.1\",\n","#         \"numpy\": \"<1.25,>=1.21\",\n","#         \"cryptography\": \"<42,>=38.0.0\",\n","#         \"numpy\": \"<1.22.2,>=1.15.0\",\n","#         \"scipy\": \"<1.8.0,>=1.7.3\",\n","#         \"fsspec\": \"==2023.12.2\",\n","#         \"typing-extensions\": \"<4.6.0,>=3.6.6\",\n","#         \"platformdirs\": \"<4,>=2.4\",\n","#         \"numpy\": \"<1.24,>=1.16.0\",\n","#         \"pandas\": \"!=1.4.0,<2.1,>1.1\",\n","#         \"scipy\": \"<1.12,>=1.4.1\",\n","#     }\n","\n","#     for package, version_spec in required_versions.items():\n","#         !pip install \"{package} {version_spec}\"\n","\n","#     # Verify that conflicts are resolved\n","#     working_set = pkg_resources.WorkingSet()\n","#     if not working_set.require(required_versions):\n","#         raise RuntimeError(\"Dependency conflicts could not be resolved.\")\n","\n","#     print(\"Dependency conflicts resolved!\")\n","\n","# if __name__ == \"__main__\":\n","#     resolve_conflicts()\n"]},{"cell_type":"markdown","metadata":{},"source":["**Importing necessary packages**"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T16:47:42.436484Z","iopub.status.busy":"2024-01-27T16:47:42.436128Z","iopub.status.idle":"2024-01-27T16:47:43.627715Z","shell.execute_reply":"2024-01-27T16:47:43.625715Z","shell.execute_reply.started":"2024-01-27T16:47:42.436450Z"},"trusted":true},"outputs":[],"source":["import os\n","import torch\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","from peft import LoraConfig\n","from trl import SFTTrainer"]},{"cell_type":"markdown","metadata":{},"source":["**Model: https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.3?text=My+name+is+Clara+and+I+am**"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.629729Z","iopub.status.idle":"2024-01-27T16:47:43.630082Z","shell.execute_reply":"2024-01-27T16:47:43.629937Z","shell.execute_reply.started":"2024-01-27T16:47:43.629921Z"},"trusted":true},"outputs":[],"source":["# Model from Hugging Face hub\n","base_model = \"PY007/TinyLlama-1.1B-Chat-v0.3\"\n","\n","# Fine-tuned model\n","new_model = \"TinyLlama-chat-medaid-model\""]},{"cell_type":"markdown","metadata":{},"source":["**Reading the PDF and storing it as a text file**"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.631566Z","iopub.status.idle":"2024-01-27T16:47:43.631903Z","shell.execute_reply":"2024-01-27T16:47:43.631730Z","shell.execute_reply.started":"2024-01-27T16:47:43.631716Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Generating train split: 53861 examples [00:00, 285108.84 examples/s]\n"]}],"source":["# import pdfplumber\n","# from datasets import load_dataset\n","\n","# # Path to your PDF file\n","# pdf_path = \"/kaggle/input/doctor-patient-conversation/combined_conversations_with_scenarios.pdf\"\n","\n","# # Extract text from PDF using pdfplumber\n","# with pdfplumber.open(pdf_path) as pdf:\n","#     text = \"\"\n","#     for page in pdf.pages:\n","#         text += page.extract_text()\n","\n","# # Save the extracted text to a file in kaggle output\n","# output_path = \"/kaggle/working/output_file_dataset.txt\"  # Use the /content directory in Colab\n","# with open(output_path, \"w\", encoding=\"utf-8\") as output_file:\n","#     output_file.write(text)\n","\n","# # Now you can load the dataset using the load_dataset function\n","# dataset = load_dataset(\"text\", data_files=output_path, split=\"train\")\n","\n","# READING THE NEW CONVERSATION DATASET WITH ALL THE TEXT FILES (doctor-patient-conversation-large)\n","\n","# Directory containing your text files\n","text_files_directory = r\"G:\\LLM-Model-MedAid-Thesis\\Model-3\\Dataset\"\n","\n","# List to store individual conversation texts\n","conversation_texts = []\n","\n","# Loop through each text file in the directory\n","for filename in os.listdir(text_files_directory):\n","    if filename.endswith(\".txt\"):\n","        file_path = os.path.join(text_files_directory, filename)\n","        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n","            # Use errors=\"ignore\" to skip characters that cannot be decoded\n","            conversation_texts.append(file.read())\n","\n","# Save the combined text to a file\n","output_path = r\"G:\\LLM-Model-MedAid-Thesis\\Model-3\\combined_conversations.txt\"\n","with open(output_path, \"w\", encoding=\"utf-8\") as output_file:\n","    for text in conversation_texts:\n","        output_file.write(text + \"\\n\\n\")\n","\n","dataset = load_dataset(\"text\", data_files=output_path, split=\"train\")\n"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.632802Z","iopub.status.idle":"2024-01-27T16:47:43.633132Z","shell.execute_reply":"2024-01-27T16:47:43.632980Z","shell.execute_reply.started":"2024-01-27T16:47:43.632965Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['text'],\n","    num_rows: 53861\n","})\n"]}],"source":["print(dataset)"]},{"cell_type":"markdown","metadata":{},"source":["**This code has a configuration for quantization using the *BitsAndBytesConfig class* from the *trl library*. Quantization is a technique used in deep learning to reduce the memory and computational requirements of neural networks.**"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.634612Z","iopub.status.idle":"2024-01-27T16:47:43.634961Z","shell.execute_reply":"2024-01-27T16:47:43.634814Z","shell.execute_reply.started":"2024-01-27T16:47:43.634794Z"},"trusted":true},"outputs":[],"source":["# compute_dtype = getattr(torch, \"float16\")\n","\n","# quant_config = BitsAndBytesConfig(\n","#     load_in_4bit=True,\n","#     bnb_4bit_quant_type=\"nf4\",\n","#     bnb_4bit_compute_dtype=compute_dtype,\n","#     bnb_4bit_use_double_quant=False,\n","# )\n","\n","import torch\n","from torch import nn\n","from torch.quantization import QuantStub, DeQuantStub, quantize_dynamic\n","\n","# Define your model\n","class MyModel(nn.Module):\n","    def __init__(self):\n","        super(MyModel, self).__init__()\n","        self.quant = QuantStub()\n","        self.dequant = DeQuantStub()\n","        self.linear = nn.Linear(1000, 10)\n","\n","    def forward(self, x):\n","        x = self.quant(x)\n","        x = self.linear(x)\n","        x = self.dequant(x)\n","        return x\n","\n","# Instantiate the model\n","model = MyModel()\n"]},{"cell_type":"markdown","metadata":{},"source":["**This code is using the AutoModelForCausalLM class from the transformers library to instantiate a pre-trained causal language model with specific configurations.**"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.1\n"]}],"source":["# %python -m bitsandbytes\n","# %pip install --upgrade bitsandbytes\n","# import torch\n","# print(torch.cuda.is_available())\n","from torch.utils.cpp_extension import CUDA_HOME\n","print(CUDA_HOME) # by default it is set to /usr/local/cuda/"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["# specify that TensorFlow performs computations using the CPU\n","import os\n","os.environ['TF_ENABLE_MLIR_OPTIMIZATIONS'] = '1'"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.636678Z","iopub.status.idle":"2024-01-27T16:47:43.637011Z","shell.execute_reply":"2024-01-27T16:47:43.636868Z","shell.execute_reply.started":"2024-01-27T16:47:43.636853Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cuda  True\n"]}],"source":["import torch\n","print(\"Cuda \", torch.cuda.is_available())\n","\n","# model = AutoModelForCausalLM.from_pretrained(\n","#     base_model,\n","#     quantization_config=quant_config,\n","#     # device_map={\"\": 0}\n","#      device_map={\"\": \"cpu\"}\n","# )\n","\n","device = torch.device('cpu')\n","model.to(device)\n","model = AutoModelForCausalLM.from_pretrained(base_model).to(device)\n","\n","\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["\n","# Quantize the model dynamically\n","quantized_model = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)"]},{"cell_type":"markdown","metadata":{},"source":["**This code is using the AutoTokenizer class from the transformers library to instantiate a tokenizer for a pre-trained language model.**"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.638414Z","iopub.status.idle":"2024-01-27T16:47:43.638745Z","shell.execute_reply":"2024-01-27T16:47:43.638600Z","shell.execute_reply.started":"2024-01-27T16:47:43.638584Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["# tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n","# tokenizer.pad_token = tokenizer.eos_token\n","# tokenizer.padding_side = \"right\"\n","\n","# Create the tokenizer on the chosen device\n","tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True, device=device)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\""]},{"cell_type":"markdown","metadata":{},"source":["**This code is defining a configuration for the LoRA (Local Reparameterization with Attention) model using the LoraConfig class, which appears to be part of the peft library.**"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.639681Z","iopub.status.idle":"2024-01-27T16:47:43.640010Z","shell.execute_reply":"2024-01-27T16:47:43.639868Z","shell.execute_reply.started":"2024-01-27T16:47:43.639853Z"},"trusted":true},"outputs":[],"source":["# Create a LoraConfig instance\n","peft_params = LoraConfig(\n","    lora_alpha=16,\n","    lora_dropout=0.1,\n","    r=64,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")"]},{"cell_type":"markdown","metadata":{},"source":["**This code is defining a set of training parameters using the TrainingArguments class, which is often used in the transformers library for configuring training settings.**"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.641661Z","iopub.status.idle":"2024-01-27T16:47:43.642104Z","shell.execute_reply":"2024-01-27T16:47:43.641905Z","shell.execute_reply.started":"2024-01-27T16:47:43.641884Z"},"trusted":true},"outputs":[],"source":["# training_params = TrainingArguments(\n","#     output_dir=\"./results\",\n","#     num_train_epochs=1,\n","#     per_device_train_batch_size=4,\n","#     gradient_accumulation_steps=1,\n","#     optim=\"paged_adamw_32bit\",\n","#     save_steps=25,\n","#     logging_steps=25,\n","#     learning_rate=2e-4,\n","#     weight_decay=0.001,\n","#     fp16=False,\n","#     bf16=False,\n","#     max_grad_norm=0.3,\n","#     max_steps=-1,\n","#     warmup_ratio=0.03,\n","#     group_by_length=True,\n","#     lr_scheduler_type=\"constant\",\n","#     report_to=\"tensorboard\"\n","# )\n","\n","# Specify the output directory and other training parameters\n","training_params = TrainingArguments(\n","    output_dir=\"./results\",\n","    num_train_epochs=1,\n","    per_device_train_batch_size=4,\n","    gradient_accumulation_steps=1,\n","    optim=\"paged_adamw_32bit\",\n","    save_steps=25,\n","    logging_steps=25,\n","    learning_rate=2e-4,\n","    weight_decay=0.001,\n","    fp16=False,\n","    bf16=False,\n","    max_grad_norm=0.3,\n","    max_steps=-1,\n","    warmup_ratio=0.03,\n","    group_by_length=True,\n","    lr_scheduler_type=\"constant\",\n","    report_to=\"tensorboard\"\n",")"]},{"cell_type":"markdown","metadata":{},"source":["**This code is creating an instance of the SFTTrainer class, presumably from the trl library, to facilitate the training of a model using the specified configuration.**\n","\n","**SFTTrainer instance is configured with the** \n","* model, \n","* training dataset, \n","* Peft configuration, \n","* tokenizer,\n","* training arguments. \n","\n","**The specific behavior and training process are determined by the SFTTrainer implementation in the trl library, and the configured parameters influence aspects such as optimization, learning rate, and model architecture during training.**"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.643115Z","iopub.status.idle":"2024-01-27T16:47:43.643436Z","shell.execute_reply":"2024-01-27T16:47:43.643292Z","shell.execute_reply.started":"2024-01-27T16:47:43.643277Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","===================================BUG REPORT===================================\n","================================================================================\n","The following directories listed in your path were found to be non-existent: {WindowsPath('C'), WindowsPath('/Users/User/anaconda3/envs/tinyLlamaMedAid/lib')}\n","The following directories listed in your path were found to be non-existent: {WindowsPath('vs/workbench/api/node/extensionHostProcess')}\n","The following directories listed in your path were found to be non-existent: {WindowsPath('file'), WindowsPath('/Users/User/anaconda3/envs/tinyLlamaMedAid/etc/xml/catalog'), WindowsPath('/C')}\n","The following directories listed in your path were found to be non-existent: {WindowsPath('module'), WindowsPath('/matplotlib_inline.backend_inline')}\n","CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n","The following directories listed in your path were found to be non-existent: {WindowsPath('/usr/local/cuda/lib64')}\n","DEBUG: Possible options found for libcudart.so: set()\n","CUDA SETUP: PyTorch settings found: CUDA_VERSION=121, Highest Compute Capability: 8.6.\n","CUDA SETUP: To manually override the PyTorch CUDA version please see:https://github.com/TimDettmers/bitsandbytes/blob/main/how_to_use_nonpytorch_cuda.md\n","CUDA SETUP: Loading binary c:\\Users\\User\\anaconda3\\envs\\tinyLlamaMedAid\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda121.so...\n","argument of type 'WindowsPath' is not iterable\n","CUDA SETUP: Problem: The main issue seems to be that the main CUDA runtime library was not detected.\n","CUDA SETUP: Solution 1: To solve the issue the libcudart.so location needs to be added to the LD_LIBRARY_PATH variable\n","CUDA SETUP: Solution 1a): Find the cuda runtime library via: find / -name libcudart.so 2>/dev/null\n","CUDA SETUP: Solution 1b): Once the library is found add it to the LD_LIBRARY_PATH: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:FOUND_PATH_FROM_1a\n","CUDA SETUP: Solution 1c): For a permanent solution add the export from 1b into your .bashrc file, located at ~/.bashrc\n","CUDA SETUP: Solution 2: If no library was found in step 1a) you need to install CUDA.\n","CUDA SETUP: Solution 2a): Download CUDA install script: wget https://raw.githubusercontent.com/TimDettmers/bitsandbytes/main/cuda_install.sh\n","CUDA SETUP: Solution 2b): Install desired CUDA version to desired location. The syntax is bash cuda_install.sh CUDA_VERSION PATH_TO_INSTALL_INTO.\n","CUDA SETUP: Solution 2b): For example, \"bash cuda_install.sh 113 ~/local/\" will download CUDA 11.3 and install into the folder ~/local\n"]},{"ename":"RuntimeError","evalue":"\n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[1;32mIn[36], line 13\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# trainer = SFTTrainer(\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#     model=model,\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#     train_dataset=dataset,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Create the trainer\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantized_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpeft_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpacking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tinyLlamaMedAid\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:210\u001b[0m, in \u001b[0;36mSFTTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size, neftune_noise_alpha, model_init_kwargs, dataset_kwargs)\u001b[0m\n\u001b[0;32m    206\u001b[0m             output\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    208\u001b[0m         model\u001b[38;5;241m.\u001b[39mget_input_embeddings()\u001b[38;5;241m.\u001b[39mregister_forward_hook(make_inputs_require_grad)\n\u001b[1;32m--> 210\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mget_peft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mbf16 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_loaded_in_4bit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    212\u001b[0m     peft_module_casting_to_bf16(model)\n","File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tinyLlamaMedAid\\lib\\site-packages\\peft\\mapping.py:137\u001b[0m, in \u001b[0;36mget_peft_model\u001b[1;34m(model, peft_config, adapter_name, mixed)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[0;32m    136\u001b[0m     peft_config \u001b[38;5;241m=\u001b[39m _prepare_prompt_learning_config(peft_config, model_config)\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tinyLlamaMedAid\\lib\\site-packages\\peft\\peft_model.py:1051\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.__init__\u001b[1;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, peft_config: PeftConfig, adapter_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1051\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation\n","File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tinyLlamaMedAid\\lib\\site-packages\\peft\\peft_model.py:127\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[1;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_peft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m PEFT_TYPE_TO_MODEL_MAPPING[peft_config\u001b[38;5;241m.\u001b[39mpeft_type]\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_gradient_checkpointing\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n","File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tinyLlamaMedAid\\lib\\site-packages\\peft\\tuners\\lora\\model.py:109\u001b[0m, in \u001b[0;36mLoraModel.__init__\u001b[1;34m(self, model, config, adapter_name)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, config, adapter_name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 109\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tinyLlamaMedAid\\lib\\site-packages\\peft\\tuners\\tuners_utils.py:148\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[1;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config\u001b[38;5;241m.\u001b[39mupdate(peft_config)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter \u001b[38;5;241m=\u001b[39m adapter_name\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpeft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config\n","File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tinyLlamaMedAid\\lib\\site-packages\\peft\\tuners\\tuners_utils.py:303\u001b[0m, in \u001b[0;36mBaseTuner.inject_adapter\u001b[1;34m(self, model, adapter_name)\u001b[0m\n\u001b[0;32m    301\u001b[0m     is_target_modules_in_base_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    302\u001b[0m     parent, target, target_name \u001b[38;5;241m=\u001b[39m _get_submodules(model, key)\n\u001b[1;32m--> 303\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_and_replace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_target_modules_in_base_model:\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget modules \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeft_config\u001b[38;5;241m.\u001b[39mtarget_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in the base model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check the target modules and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    309\u001b[0m     )\n","File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tinyLlamaMedAid\\lib\\site-packages\\peft\\tuners\\lora\\model.py:176\u001b[0m, in \u001b[0;36mLoraModel._create_and_replace\u001b[1;34m(self, lora_config, adapter_name, target, target_name, parent, current_key)\u001b[0m\n\u001b[0;32m    167\u001b[0m     target\u001b[38;5;241m.\u001b[39mupdate_layer(\n\u001b[0;32m    168\u001b[0m         adapter_name,\n\u001b[0;32m    169\u001b[0m         r,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    173\u001b[0m         lora_config\u001b[38;5;241m.\u001b[39muse_rslora,\n\u001b[0;32m    174\u001b[0m     )\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m     new_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m adapter_name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter:\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;66;03m# adding an additional adapter: it is not automatically trainable\u001b[39;00m\n\u001b[0;32m    179\u001b[0m         new_module\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n","File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tinyLlamaMedAid\\lib\\site-packages\\peft\\tuners\\lora\\model.py:238\u001b[0m, in \u001b[0;36mLoraModel._create_new_module\u001b[1;34m(lora_config, adapter_name, target, **kwargs)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;66;03m# avoid eager bnb import\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_bnb_available():\n\u001b[1;32m--> 238\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbnb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch_bnb_8bit\n\u001b[0;32m    240\u001b[0m     dispatchers\u001b[38;5;241m.\u001b[39mappend(dispatch_bnb_8bit)\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_bnb_4bit_available():\n","File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tinyLlamaMedAid\\lib\\site-packages\\peft\\tuners\\lora\\bnb.py:19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Optional\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mbnb\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_bnb_4bit_available, is_bnb_available\n","File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tinyLlamaMedAid\\lib\\site-packages\\bitsandbytes\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cuda_setup, utils, research\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     MatmulLtState,\n\u001b[0;32m      9\u001b[0m     bmm_cublas,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     matmul_4bit\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m COMPILED_WITH_CUDA\n","File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tinyLlamaMedAid\\lib\\site-packages\\bitsandbytes\\research\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     switchback_bnb,\n\u001b[0;32m      4\u001b[0m     matmul_fp8_global,\n\u001b[0;32m      5\u001b[0m     matmul_fp8_mixed,\n\u001b[0;32m      6\u001b[0m )\n","File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tinyLlamaMedAid\\lib\\site-packages\\bitsandbytes\\research\\nn\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearFP8Mixed, LinearFP8Global\n","File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tinyLlamaMedAid\\lib\\site-packages\\bitsandbytes\\research\\nn\\modules.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor, device, dtype, nn\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mbnb\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GlobalOptimManager\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OutlierTracer, find_outlier_dims\n\u001b[0;32m     11\u001b[0m T \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, bound\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.nn.Module\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tinyLlamaMedAid\\lib\\site-packages\\bitsandbytes\\optim\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m COMPILED_WITH_CUDA\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madagrad\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adagrad, Adagrad8bit, Adagrad32bit\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madam\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam, Adam8bit, Adam32bit, PagedAdam, PagedAdam8bit, PagedAdam32bit\n","File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\tinyLlamaMedAid\\lib\\site-packages\\bitsandbytes\\cextension.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m     CUDASetup\u001b[38;5;241m.\u001b[39mget_instance()\u001b[38;5;241m.\u001b[39mgenerate_instructions()\n\u001b[0;32m     19\u001b[0m     CUDASetup\u001b[38;5;241m.\u001b[39mget_instance()\u001b[38;5;241m.\u001b[39mprint_log_stack()\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;124m    CUDA Setup failed despite GPU being available. Please run the following command to get more information:\u001b[39m\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m \u001b[38;5;124m    python -m bitsandbytes\u001b[39m\n\u001b[0;32m     24\u001b[0m \n\u001b[0;32m     25\u001b[0m \u001b[38;5;124m    Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\u001b[39m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124m    to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\u001b[39m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124m    and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues\u001b[39m\u001b[38;5;124m'''\u001b[39m)\n\u001b[0;32m     28\u001b[0m lib\u001b[38;5;241m.\u001b[39mcadam32bit_grad_fp32 \u001b[38;5;66;03m# runs on an error if the library could not be found -> COMPILED_WITH_CUDA=False\u001b[39;00m\n\u001b[0;32m     29\u001b[0m lib\u001b[38;5;241m.\u001b[39mget_context\u001b[38;5;241m.\u001b[39mrestype \u001b[38;5;241m=\u001b[39m ct\u001b[38;5;241m.\u001b[39mc_void_p\n","\u001b[1;31mRuntimeError\u001b[0m: \n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues"]}],"source":["# trainer = SFTTrainer(\n","#     model=model,\n","#     train_dataset=dataset,\n","#     peft_config=peft_params,\n","#     dataset_text_field=\"text\",\n","#     max_seq_length=None,\n","#     tokenizer=tokenizer,\n","#     args=training_params,\n","#     packing=False,\n","# )\n","\n","# Create the trainer\n","trainer = SFTTrainer(\n","    model=quantized_model,\n","    train_dataset=dataset,\n","    peft_config=peft_params,\n","    dataset_text_field=\"text\",\n","    max_seq_length=None,\n","    tokenizer=tokenizer,\n","    args=training_params,\n","    packing=False,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.644647Z","iopub.status.idle":"2024-01-27T16:47:43.644991Z","shell.execute_reply":"2024-01-27T16:47:43.644847Z","shell.execute_reply.started":"2024-01-27T16:47:43.644832Z"},"trusted":true},"outputs":[],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["**The code you provided is saving the trained model and its associated tokenizer to a specified directory using the save_pretrained method.**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.646365Z","iopub.status.idle":"2024-01-27T16:47:43.646701Z","shell.execute_reply":"2024-01-27T16:47:43.646547Z","shell.execute_reply.started":"2024-01-27T16:47:43.646531Z"},"trusted":true},"outputs":[],"source":["trainer.model.save_pretrained(new_model)\n","trainer.tokenizer.save_pretrained(new_model)"]},{"cell_type":"markdown","metadata":{},"source":["**This code creates a simple conversational loop that simulates a doctor assistant interaction. It uses a fine-tuned language model (assumed to be a text generation model) and a tokenizer to generate responses based on user input. The conversation is logged in a text file, and the loop continues until the user provides an exit signal.**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.647706Z","iopub.status.idle":"2024-01-27T16:47:43.648052Z","shell.execute_reply":"2024-01-27T16:47:43.647908Z","shell.execute_reply.started":"2024-01-27T16:47:43.647892Z"},"trusted":true},"outputs":[],"source":["# logging.set_verbosity(logging.CRITICAL)\n","\n","# Initial prompt\n","prompt = \"Be a doctor assistant. And keep questioning one by one to extract symptoms and history of the patient. Don't give advice or ask anything else. Just extract symptoms or history by questioning one question at a time.\"\n","\n","# Create a pipeline for text generation using the fine-tuned model and tokenizer\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n","\n","# File to save the conversation\n","output_file_path = \"G:\\LLM-Model-MedAid-Thesis\\Model-3\\conversation_log.txt\"  # Update 'your_folder' with the desired folder in your Drive\n","\n","# Function to ask a question and get the user's response\n","def ask_question_and_log(prompt, user_response, file_path):\n","    # Ask the question\n","    result = pipe(f\"<s>[Prompt] {prompt} \\n [User response] {user_response} \\n [/Response]\")\n","\n","    # Get the generated text (question)\n","    generated_text = result[0]['generated_text']\n","\n","    # Print and save the generated text (question)\n","    print(generated_text)\n","    with open(file_path, \"a\", encoding=\"utf-8\") as output_file:\n","        output_file.write(f\"[Model] {generated_text}\\n\\n\")\n","\n","    # Simulate user answering the question\n","    user_response = input(\"Your response: \")  # User provides input\n","\n","    return user_response\n","\n","# Initial user response\n","user_response = \"I have chest pain.\"\n","\n","# Ask a question based on the user's response and log the conversation\n","with open(output_file_path, \"a\", encoding=\"utf-8\") as output_file:\n","    output_file.write(\"\\nConversation started.\\n\\n\")\n","\n","# Loop to continue the conversation\n","while True:\n","    user_response = ask_question_and_log(prompt, user_response, output_file_path)\n","\n","    # Check for an exit condition (e.g., user response indicating the end of the conversation)\n","    if \"exit\" in user_response.lower():\n","        print(\"Ending the conversation.\")\n","\n","        # Save the conversation to a file\n","        with open(output_file_path, \"a\", encoding=\"utf-8\") as output_file:\n","            output_file.write(\"\\nConversation ended by user. ---------------- \\n\\n\")\n","\n","        break\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4256693,"sourceId":7332627,"sourceType":"datasetVersion"},{"datasetId":4354465,"sourceId":7480410,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"tinyLlamaMedAid","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":4}
