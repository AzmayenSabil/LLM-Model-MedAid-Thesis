{"cells":[{"cell_type":"markdown","metadata":{},"source":["INSTALL PACKAGES"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T16:47:12.027287Z","iopub.status.busy":"2024-01-27T16:47:12.026981Z","iopub.status.idle":"2024-01-27T16:47:42.426119Z","shell.execute_reply":"2024-01-27T16:47:42.425125Z","shell.execute_reply.started":"2024-01-27T16:47:12.027264Z"},"trusted":true},"outputs":[],"source":["# !pip install --upgrade accelerate peft bitsandbytes transformers pdfplumber datasets==2.16.1 trl==0.7.9 --target=/kaggle/working/packages\n","%pip install --upgrade accelerate peft transformers pdfplumber datasets==2.16.1 trl\n","%pip install bitsandbytes --prefer-binary --extra-index-url=https://jllllll.github.io/bitsandbytes-windows-webui\n"]},{"cell_type":"markdown","metadata":{},"source":["**To resolve some issues while installing packages**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T16:47:42.428523Z","iopub.status.busy":"2024-01-27T16:47:42.427733Z","iopub.status.idle":"2024-01-27T16:47:42.434850Z","shell.execute_reply":"2024-01-27T16:47:42.433821Z","shell.execute_reply.started":"2024-01-27T16:47:42.428486Z"},"trusted":true},"outputs":[],"source":["# import pkg_resources\n","\n","# def resolve_conflicts():\n","#     \"\"\"\n","#     Resolves dependency conflicts by creating a new environment and installing compatible versions.\n","#     \"\"\"\n","\n","#     # Create a new virtual environment (replace \"myvenv\" with your desired name)\n","#     !virtualenv myvenv\n","#     !source myvenv/bin/activate\n","\n","#     # Install compatible versions of conflicting packages\n","#     required_versions = {\n","#         \"cupy-cuda11x\": \">=12.0.0\",\n","#         \"dill\": \"<0.3.2,>=0.3.1.1\",\n","#         \"numpy\": \"<1.25.0,>=1.14.3\",\n","#         \"pyarrow\": \"<10.0.0,>=3.0.0\",\n","#         \"jupyter-server\": \"~=1.16\",\n","#         \"jupyterlab\": \"~=3.4\",\n","#         \"urllib3\": \"<2.1,>=1.25.4\",  # Ensure python_version >= \"3.10\"\n","#         \"pandas\": \"<1.6.0dev0,>=1.3\",\n","#         \"protobuf\": \"<5,>=4.21\",\n","#         \"dask\": \"==2023.7.1\",\n","#         \"distributed\": \"==2023.7.1\",\n","#         \"fsspec\": \"==2023.6.0\",\n","#         \"urllib3\": \"<2.0\",\n","#         \"google-api-core[grpc]\": \"<2.0.0dev,>=1.22.2\",\n","#         \"packaging\": \"<22.0dev,>=14.3\",\n","#         \"grpcio\": \"<2.0dev,>=1.51.3\",\n","#         \"jupyter-lsp\": \">=2.0.0\",\n","#         \"google-cloud-storage\": \"<3,>=2.2.1\",\n","#         \"shapely\": \">=2.0.1\",\n","#         \"numpy\": \"<1.25,>=1.21\",\n","#         \"cryptography\": \"<42,>=38.0.0\",\n","#         \"numpy\": \"<1.22.2,>=1.15.0\",\n","#         \"scipy\": \"<1.8.0,>=1.7.3\",\n","#         \"fsspec\": \"==2023.12.2\",\n","#         \"typing-extensions\": \"<4.6.0,>=3.6.6\",\n","#         \"platformdirs\": \"<4,>=2.4\",\n","#         \"numpy\": \"<1.24,>=1.16.0\",\n","#         \"pandas\": \"!=1.4.0,<2.1,>1.1\",\n","#         \"scipy\": \"<1.12,>=1.4.1\",\n","#     }\n","\n","#     for package, version_spec in required_versions.items():\n","#         !pip install \"{package} {version_spec}\"\n","\n","#     # Verify that conflicts are resolved\n","#     working_set = pkg_resources.WorkingSet()\n","#     if not working_set.require(required_versions):\n","#         raise RuntimeError(\"Dependency conflicts could not be resolved.\")\n","\n","#     print(\"Dependency conflicts resolved!\")\n","\n","# if __name__ == \"__main__\":\n","#     resolve_conflicts()\n"]},{"cell_type":"markdown","metadata":{},"source":["**Importing necessary packages**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T16:47:42.436484Z","iopub.status.busy":"2024-01-27T16:47:42.436128Z","iopub.status.idle":"2024-01-27T16:47:43.627715Z","shell.execute_reply":"2024-01-27T16:47:43.625715Z","shell.execute_reply.started":"2024-01-27T16:47:42.436450Z"},"trusted":true},"outputs":[],"source":["import os\n","import torch\n","import transformers\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","from peft import prepare_model_for_kbit_training\n","from peft import LoraConfig, get_peft_model\n","from trl import SFTTrainer"]},{"cell_type":"markdown","metadata":{},"source":["**Model: https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.3?text=My+name+is+Clara+and+I+am**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.629729Z","iopub.status.idle":"2024-01-27T16:47:43.630082Z","shell.execute_reply":"2024-01-27T16:47:43.629937Z","shell.execute_reply.started":"2024-01-27T16:47:43.629921Z"},"trusted":true},"outputs":[],"source":["# Model from Hugging Face hub\n","base_model = \"PY007/TinyLlama-1.1B-Chat-v0.3\"\n","\n","# Fine-tuned model\n","new_model = \"TinyLlama-chat-medaid-model\""]},{"cell_type":"markdown","metadata":{},"source":["**Reading the PDF and storing it as a text file**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.631566Z","iopub.status.idle":"2024-01-27T16:47:43.631903Z","shell.execute_reply":"2024-01-27T16:47:43.631730Z","shell.execute_reply.started":"2024-01-27T16:47:43.631716Z"},"trusted":true},"outputs":[],"source":["# READING THE NEW CONVERSATION DATASET WITH ALL THE TEXT FILES (doctor-patient-conversation-large)\n","\n","# Directory containing your text files\n","text_files_directory = r\"G:\\LLM-Model-MedAid-Thesis\\Model-3\\Dataset\"\n","\n","# List to store individual conversation texts\n","conversation_texts = []\n","\n","# Loop through each text file in the directory\n","for filename in os.listdir(text_files_directory):\n","    if filename.endswith(\".txt\"):\n","        file_path = os.path.join(text_files_directory, filename)\n","        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n","            # Use errors=\"ignore\" to skip characters that cannot be decoded\n","            conversation_texts.append(file.read())\n","\n","# Save the combined text to a file\n","output_path = r\"G:\\LLM-Model-MedAid-Thesis\\Model-3\\combined_conversations.txt\"\n","with open(output_path, \"w\", encoding=\"utf-8\") as output_file:\n","    for text in conversation_texts:\n","        output_file.write(text + \"\\n\\n\")\n","\n","dataset = load_dataset(\"text\", data_files=output_path, split=\"train\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.632802Z","iopub.status.idle":"2024-01-27T16:47:43.633132Z","shell.execute_reply":"2024-01-27T16:47:43.632980Z","shell.execute_reply.started":"2024-01-27T16:47:43.632965Z"},"trusted":true},"outputs":[],"source":["print(dataset)"]},{"cell_type":"markdown","metadata":{},"source":["**This code has a configuration for quantization using the *BitsAndBytesConfig class* from the *trl library*. Quantization is a technique used in deep learning to reduce the memory and computational requirements of neural networks.**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.634612Z","iopub.status.idle":"2024-01-27T16:47:43.634961Z","shell.execute_reply":"2024-01-27T16:47:43.634814Z","shell.execute_reply.started":"2024-01-27T16:47:43.634794Z"},"trusted":true},"outputs":[],"source":["compute_dtype = getattr(torch, \"float16\")\n","\n","quant_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    # bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_compute_dtype=torch.bfloat16,\n","    bnb_4bit_use_double_quant=False,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","from torch.utils.cpp_extension import CUDA_HOME\n","print(CUDA_HOME) # by default it is set to /usr/local/cuda/"]},{"cell_type":"markdown","metadata":{},"source":["**This code is using the AutoModelForCausalLM class from the transformers library to instantiate a pre-trained causal language model with specific configurations.**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.636678Z","iopub.status.idle":"2024-01-27T16:47:43.637011Z","shell.execute_reply":"2024-01-27T16:47:43.636868Z","shell.execute_reply.started":"2024-01-27T16:47:43.636853Z"},"trusted":true},"outputs":[],"source":["import torch\n","print(\"Cuda \", torch.cuda.is_available())\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    base_model,\n","    quantization_config=quant_config,\n","    # device_map={\"\": 0}\n","    device_map=\"auto\", # automatically figures out how to best use CPU + GPU for loading model\n","    trust_remote_code=True, # prevents running custom model files on your machine\n",")\n","\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n"]},{"cell_type":"markdown","metadata":{},"source":["**This code is using the AutoTokenizer class from the transformers library to instantiate a tokenizer for a pre-trained language model.**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.638414Z","iopub.status.idle":"2024-01-27T16:47:43.638745Z","shell.execute_reply":"2024-01-27T16:47:43.638600Z","shell.execute_reply.started":"2024-01-27T16:47:43.638584Z"},"trusted":true},"outputs":[],"source":["\n","# Create the tokenizer on the chosen device\n","# tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n","tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\""]},{"cell_type":"markdown","metadata":{},"source":["Prepare Model for Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.train() # model in training mode (dropout modules are activated)\n","\n","# enable gradient check pointing\n","model.gradient_checkpointing_enable()\n","\n","# enable quantized training\n","model = prepare_model_for_kbit_training(model)"]},{"cell_type":"markdown","metadata":{},"source":["**This code is defining a configuration for the LoRA (Local Reparameterization with Attention) model using the LoraConfig class, which appears to be part of the peft library.**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.639681Z","iopub.status.idle":"2024-01-27T16:47:43.640010Z","shell.execute_reply":"2024-01-27T16:47:43.639868Z","shell.execute_reply.started":"2024-01-27T16:47:43.639853Z"},"trusted":true},"outputs":[],"source":["# Create a LoraConfig instance\n","peft_params = LoraConfig(\n","    r=8,\n","    lora_alpha=32,\n","    target_modules=[\"q_proj\"],\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","# LoRA trainable version of model\n","model = get_peft_model(model, peft_params)\n","\n","# trainable parameter count\n","model.print_trainable_parameters()"]},{"cell_type":"markdown","metadata":{},"source":["**Preparing Training Dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # create tokenize function\n","# def tokenize_function(examples):\n","#     # extract text\n","#     text = examples[\"example\"]\n","\n","#     #tokenize and truncate text\n","#     tokenizer.truncation_side = \"left\"\n","#     tokenized_inputs = tokenizer(\n","#         text,\n","#         return_tensors=\"np\",\n","#         truncation=True,\n","#         max_length=512\n","#     )\n","\n","#     return tokenized_inputs\n","\n","# # tokenize training and validation datasets\n","# tokenized_data = dataset.map(tokenize_function, batched=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# setting pad token\n","tokenizer.pad_token = tokenizer.eos_token\n","# data collator\n","data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)"]},{"cell_type":"markdown","metadata":{},"source":["**This code is defining a set of training parameters using the TrainingArguments class, which is often used in the transformers library for configuring training settings.**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.641661Z","iopub.status.idle":"2024-01-27T16:47:43.642104Z","shell.execute_reply":"2024-01-27T16:47:43.641905Z","shell.execute_reply.started":"2024-01-27T16:47:43.641884Z"},"trusted":true},"outputs":[],"source":["# hyperparameters\n","lr = 2e-4\n","batch_size = 4\n","num_epochs = 10\n","\n","# Specify the output directory and other training parameters\n","training_params = TrainingArguments(\n","    output_dir=\"./results\",\n","    learning_rate=lr,\n","    per_device_train_batch_size=batch_size,\n","    num_train_epochs=num_epochs,\n","    gradient_accumulation_steps=4,\n","    # save_steps=25,\n","    logging_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    # logging_steps=25,  \n","    weight_decay=0.01,\n","    # load_best_model_at_end=True,\n","    warmup_steps=2,\n","    fp16=True,\n","    optim=\"paged_adamw_8bit\",\n","    # bf16=False,\n","    # max_grad_norm=0.3,\n","    # max_steps=-1,\n","    # warmup_ratio=0.03,\n","    # group_by_length=True,\n","    # lr_scheduler_type=\"constant\",\n","    # report_to=\"tensorboard\"\n",")"]},{"cell_type":"markdown","metadata":{},"source":["**This code is creating an instance of the SFTTrainer class, presumably from the trl library, to facilitate the training of a model using the specified configuration.**\n","\n","**SFTTrainer instance is configured with the** \n","* model, \n","* training dataset, \n","* Peft configuration, \n","* tokenizer,\n","* training arguments. \n","\n","**The specific behavior and training process are determined by the SFTTrainer implementation in the trl library, and the configured parameters influence aspects such as optimization, learning rate, and model architecture during training.**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.643115Z","iopub.status.idle":"2024-01-27T16:47:43.643436Z","shell.execute_reply":"2024-01-27T16:47:43.643292Z","shell.execute_reply.started":"2024-01-27T16:47:43.643277Z"},"trusted":true},"outputs":[],"source":["# Create the trainer\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_params,\n","    dataset_text_field=\"text\",\n","    max_seq_length=None,\n","    tokenizer=tokenizer,\n","    args=training_params,\n","    packing=False,\n","    data_collator=data_collator\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.644647Z","iopub.status.idle":"2024-01-27T16:47:43.644991Z","shell.execute_reply":"2024-01-27T16:47:43.644847Z","shell.execute_reply.started":"2024-01-27T16:47:43.644832Z"},"trusted":true},"outputs":[],"source":["# train model\n","model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n","trainer.train()\n","\n","# renable warnings\n","model.config.use_cache = True"]},{"cell_type":"markdown","metadata":{},"source":["**The code you provided is saving the trained model and its associated tokenizer to a specified directory using the save_pretrained method.**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_path = \"preTrained_model\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.646365Z","iopub.status.idle":"2024-01-27T16:47:43.646701Z","shell.execute_reply":"2024-01-27T16:47:43.646547Z","shell.execute_reply.started":"2024-01-27T16:47:43.646531Z"},"trusted":true},"outputs":[],"source":["trainer.save_model(model_path)\n","tokenizer.save_pretrained(model_path)\n","\n","# trainer.model.save_pretrained(new_model)\n","# trainer.tokenizer.save_pretrained(new_model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import the pretrained model\n","model = AutoModelForCausalLM.from_pretrained(model_path)\n","tokenizer = AutoTokenizer.from_pretrained(model_path)"]},{"cell_type":"markdown","metadata":{},"source":["**This code creates a simple conversational loop that simulates a doctor assistant interaction. It uses a fine-tuned language model (assumed to be a text generation model) and a tokenizer to generate responses based on user input. The conversation is logged in a text file, and the loop continues until the user provides an exit signal.**"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.647706Z","iopub.status.idle":"2024-01-27T16:47:43.648052Z","shell.execute_reply":"2024-01-27T16:47:43.647908Z","shell.execute_reply.started":"2024-01-27T16:47:43.647892Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Both `max_new_tokens` (=32) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"name":"stdout","output_type":"stream","text":["<s>[Prompt] Be a doctor assistant. And keep questioning one by one to extract symptoms and history of the patient. Don't give advice or ask anything else. Just extract symptoms or history by questioning one question at a time. \n"," [User response] I have chest pain. \n"," [/Response] [User response] [User response] [User response] [User response] [User response] [User response] [User response] [User response]\n"]},{"name":"stderr","output_type":"stream","text":["Both `max_new_tokens` (=32) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"name":"stdout","output_type":"stream","text":["<s>[Prompt] Be a doctor assistant. And keep questioning one by one to extract symptoms and history of the patient. Don't give advice or ask anything else. Just extract symptoms or history by questioning one question at a time. \n"," [User response] When I take a deep breath that the pain is a lot worse. \n"," [/Response] [/Question]\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"name":"stderr","output_type":"stream","text":["Both `max_new_tokens` (=32) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"name":"stdout","output_type":"stream","text":["<s>[Prompt] Be a doctor assistant. And keep questioning one by one to extract symptoms and history of the patient. Don't give advice or ask anything else. Just extract symptoms or history by questioning one question at a time. \n"," [User response]  \n"," [/Response] \n"," [Prompt] [User response] \n"," [User response] \n"," [Prompt] [User response] \n"," [User response]\n","Ending the conversation.\n"]}],"source":["# logging.set_verbosity(logging.CRITICAL)\n","\n","# Initial prompt\n","prompt = \"Be a doctor assistant. And keep questioning one by one to extract symptoms and history of the patient. Don't give advice or ask anything else. Just extract symptoms or history by questioning one question at a time.\"\n","\n","# Create a pipeline for text generation using the fine-tuned model and tokenizer\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n","\n","# File to save the conversation\n","output_file_path = \"G:\\LLM-Model-MedAid-Thesis\\Model-3\\conversation_log.txt\"  # Update 'your_folder' with the desired folder in your Drive\n","\n","# Function to ask a question and get the user's response\n","def ask_question_and_log(prompt, user_response, file_path):\n","    # Ask the question\n","    result = pipe(f\"<s>[Prompt] {prompt} \\n [User response] {user_response} \\n [/Response]\")\n","\n","    # Get the generated text (question)\n","    generated_text = result[0]['generated_text']\n","\n","    # Print and save the generated text (question)\n","    print(generated_text)\n","    with open(file_path, \"a\", encoding=\"utf-8\") as output_file:\n","        output_file.write(f\"[Model] {generated_text}\\n\\n\")\n","\n","    # Simulate user answering the question\n","    user_response = input(\"Your response: \")  # User provides input\n","\n","    return user_response\n","\n","# Initial user response\n","user_response = \"I have chest pain.\"\n","\n","# Ask a question based on the user's response and log the conversation\n","with open(output_file_path, \"a\", encoding=\"utf-8\") as output_file:\n","    output_file.write(\"\\nConversation started.\\n\\n\")\n","\n","# Loop to continue the conversation\n","while True:\n","    user_response = ask_question_and_log(prompt, user_response, output_file_path)\n","\n","    # Check for an exit condition (e.g., user response indicating the end of the conversation)\n","    if \"exit\" in user_response.lower():\n","        print(\"Ending the conversation.\")\n","\n","        # Save the conversation to a file\n","        with open(output_file_path, \"a\", encoding=\"utf-8\") as output_file:\n","            output_file.write(\"\\nConversation ended by user. ---------------- \\n\\n\")\n","\n","        break\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4256693,"sourceId":7332627,"sourceType":"datasetVersion"},{"datasetId":4354465,"sourceId":7480410,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"tinyLlamaMedAid","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.1"}},"nbformat":4,"nbformat_minor":4}
