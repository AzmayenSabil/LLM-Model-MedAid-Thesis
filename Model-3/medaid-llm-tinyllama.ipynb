{"cells":[{"cell_type":"markdown","metadata":{},"source":["INSTALL PACKAGES"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T16:47:12.027287Z","iopub.status.busy":"2024-01-27T16:47:12.026981Z","iopub.status.idle":"2024-01-27T16:47:42.426119Z","shell.execute_reply":"2024-01-27T16:47:42.425125Z","shell.execute_reply.started":"2024-01-27T16:47:12.027264Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: accelerate in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (0.26.1)\n","Collecting accelerate\n","  Using cached accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: peft in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (0.7.1)\n","Collecting peft\n","  Using cached peft-0.9.0-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: transformers in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (4.36.2)\n","Collecting transformers\n","  Using cached transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n","Requirement already satisfied: pdfplumber in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (0.10.3)\n","Collecting pdfplumber\n","  Using cached pdfplumber-0.11.0-py3-none-any.whl.metadata (39 kB)\n","Requirement already satisfied: datasets==2.16.1 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (2.16.1)\n","Requirement already satisfied: trl in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (0.7.9)\n","Collecting trl\n","  Using cached trl-0.7.11-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: filelock in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from datasets==2.16.1) (3.12.2)\n","Requirement already satisfied: numpy>=1.17 in c:\\python310\\lib\\site-packages (from datasets==2.16.1) (1.24.2)\n","Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from datasets==2.16.1) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from datasets==2.16.1) (0.6)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from datasets==2.16.1) (0.3.7)\n","Requirement already satisfied: pandas in c:\\python310\\lib\\site-packages (from datasets==2.16.1) (2.0.3)\n","Requirement already satisfied: requests>=2.19.0 in c:\\python310\\lib\\site-packages (from datasets==2.16.1) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in c:\\python310\\lib\\site-packages (from datasets==2.16.1) (4.66.1)\n","Requirement already satisfied: xxhash in c:\\python310\\lib\\site-packages (from datasets==2.16.1) (3.4.1)\n","Requirement already satisfied: multiprocess in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from datasets==2.16.1) (0.70.15)\n","Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.16.1) (2023.6.0)\n","Requirement already satisfied: aiohttp in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from datasets==2.16.1) (3.9.1)\n","Requirement already satisfied: huggingface-hub>=0.19.4 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from datasets==2.16.1) (0.20.2)\n","Requirement already satisfied: packaging in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from datasets==2.16.1) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from datasets==2.16.1) (6.0.1)\n","Requirement already satisfied: psutil in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from accelerate) (5.9.3)\n","Requirement already satisfied: torch>=1.10.0 in c:\\python310\\lib\\site-packages (from accelerate) (2.1.2+cu118)\n","Requirement already satisfied: safetensors>=0.3.1 in c:\\python310\\lib\\site-packages (from accelerate) (0.4.2)\n","Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2023.8.8)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.15.0)\n","Collecting pdfminer.six==20231228 (from pdfplumber)\n","  Using cached pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n","Requirement already satisfied: Pillow>=9.1 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from pdfplumber) (10.2.0)\n","Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from pdfplumber) (4.26.0)\n","Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\python310\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (3.0.1)\n","Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from pdfminer.six==20231228->pdfplumber) (41.0.7)\n","Requirement already satisfied: tyro>=0.5.11 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from trl) (0.6.3)\n","Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets==2.16.1) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets==2.16.1) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets==2.16.1) (1.9.4)\n","Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets==2.16.1) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets==2.16.1) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets==2.16.1) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\python310\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets==2.16.1) (4.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from packaging->datasets==2.16.1) (3.0.9)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests>=2.19.0->datasets==2.16.1) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests>=2.19.0->datasets==2.16.1) (2.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests>=2.19.0->datasets==2.16.1) (2022.12.7)\n","Requirement already satisfied: sympy in c:\\python310\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in c:\\python310\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.62.1->datasets==2.16.1) (0.4.5)\n","Requirement already satisfied: docstring-parser>=0.14.1 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from tyro>=0.5.11->trl) (0.15)\n","Requirement already satisfied: rich>=11.1.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from tyro>=0.5.11->trl) (13.7.0)\n","Requirement already satisfied: shtab>=1.5.6 in c:\\python310\\lib\\site-packages (from tyro>=0.5.11->trl) (1.6.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from pandas->datasets==2.16.1) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in c:\\python310\\lib\\site-packages (from pandas->datasets==2.16.1) (2022.7.1)\n","Requirement already satisfied: tzdata>=2022.1 in c:\\python310\\lib\\site-packages (from pandas->datasets==2.16.1) (2023.3)\n","Requirement already satisfied: cffi>=1.12 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.16.0)\n","Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.16.1) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.13.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in c:\\python310\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in c:\\python310\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Requirement already satisfied: pycparser in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.21)\n","Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n","Using cached accelerate-0.28.0-py3-none-any.whl (290 kB)\n","Using cached peft-0.9.0-py3-none-any.whl (190 kB)\n","Using cached transformers-4.38.2-py3-none-any.whl (8.5 MB)\n","Using cached pdfplumber-0.11.0-py3-none-any.whl (56 kB)\n","Using cached pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n","Using cached trl-0.7.11-py3-none-any.whl (155 kB)\n","Installing collected packages: pdfminer.six, accelerate, transformers, pdfplumber, trl, peft\n","  Attempting uninstall: pdfminer.six\n","    Found existing installation: pdfminer.six 20221105\n","    Uninstalling pdfminer.six-20221105:\n","      Successfully uninstalled pdfminer.six-20221105\n","  Rolling back uninstall of pdfminer.six\n","  Moving to c:\\users\\user\\appdata\\roaming\\python\\python310\\scripts\\__pycache__\\dumppdf.cpython-310.pyc\n","   from C:\\Users\\User\\AppData\\Local\\Temp\\pip-uninstall-l24u64ul\\dumppdf.cpython-310.pyc\n","  Moving to c:\\users\\user\\appdata\\roaming\\python\\python310\\scripts\\__pycache__\\pdf2txt.cpython-310.pyc\n","   from C:\\Users\\User\\AppData\\Local\\Temp\\pip-uninstall-l24u64ul\\pdf2txt.cpython-310.pyc\n","  Moving to c:\\users\\user\\appdata\\roaming\\python\\python310\\scripts\\dumppdf.py\n","   from C:\\Users\\User\\AppData\\Local\\Temp\\pip-uninstall-84lbjm5i\\dumppdf.py\n","  Moving to c:\\users\\user\\appdata\\roaming\\python\\python310\\scripts\\pdf2txt.py\n","   from C:\\Users\\User\\AppData\\Local\\Temp\\pip-uninstall-84lbjm5i\\pdf2txt.py\n","  Moving to c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages\\pdfminer.six-20221105.dist-info\\\n","   from C:\\Users\\User\\AppData\\Roaming\\Python\\Python310\\site-packages\\~dfminer.six-20221105.dist-info\n","  Moving to c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages\\pdfminer\\\n","   from C:\\Users\\User\\AppData\\Roaming\\Python\\Python310\\site-packages\\~dfminer\n","Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["ERROR: Could not install packages due to an OSError: [Errno 13] Permission denied: 'c:\\\\Python310\\\\Scripts\\\\dumppdf.py'\n","Consider using the `--user` option or check the permissions.\n","\n","\n","[notice] A new release of pip is available: 23.3.2 -> 24.0\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]},{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://jllllll.github.io/bitsandbytes-windows-webui\n","Requirement already satisfied: bitsandbytes in c:\\python310\\lib\\site-packages (0.41.1)\n","Requirement already satisfied: scipy in c:\\python310\\lib\\site-packages (from bitsandbytes) (1.11.2)\n","Requirement already satisfied: numpy<1.28.0,>=1.21.6 in c:\\python310\\lib\\site-packages (from scipy->bitsandbytes) (1.24.2)\n","Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip is available: 23.3.2 -> 24.0\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]}],"source":["# !pip install --upgrade accelerate peft bitsandbytes transformers pdfplumber datasets==2.16.1 trl==0.7.9 --target=/kaggle/working/packages\n","%pip install --upgrade accelerate peft transformers pdfplumber datasets==2.16.1 trl\n","%pip install bitsandbytes --prefer-binary --extra-index-url=https://jllllll.github.io/bitsandbytes-windows-webui\n"]},{"cell_type":"markdown","metadata":{},"source":["**To resolve some issues while installing packages**"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T16:47:42.428523Z","iopub.status.busy":"2024-01-27T16:47:42.427733Z","iopub.status.idle":"2024-01-27T16:47:42.434850Z","shell.execute_reply":"2024-01-27T16:47:42.433821Z","shell.execute_reply.started":"2024-01-27T16:47:42.428486Z"},"trusted":true},"outputs":[],"source":["# import pkg_resources\n","\n","# def resolve_conflicts():\n","#     \"\"\"\n","#     Resolves dependency conflicts by creating a new environment and installing compatible versions.\n","#     \"\"\"\n","\n","#     # Create a new virtual environment (replace \"myvenv\" with your desired name)\n","#     !virtualenv myvenv\n","#     !source myvenv/bin/activate\n","\n","#     # Install compatible versions of conflicting packages\n","#     required_versions = {\n","#         \"cupy-cuda11x\": \">=12.0.0\",\n","#         \"dill\": \"<0.3.2,>=0.3.1.1\",\n","#         \"numpy\": \"<1.25.0,>=1.14.3\",\n","#         \"pyarrow\": \"<10.0.0,>=3.0.0\",\n","#         \"jupyter-server\": \"~=1.16\",\n","#         \"jupyterlab\": \"~=3.4\",\n","#         \"urllib3\": \"<2.1,>=1.25.4\",  # Ensure python_version >= \"3.10\"\n","#         \"pandas\": \"<1.6.0dev0,>=1.3\",\n","#         \"protobuf\": \"<5,>=4.21\",\n","#         \"dask\": \"==2023.7.1\",\n","#         \"distributed\": \"==2023.7.1\",\n","#         \"fsspec\": \"==2023.6.0\",\n","#         \"urllib3\": \"<2.0\",\n","#         \"google-api-core[grpc]\": \"<2.0.0dev,>=1.22.2\",\n","#         \"packaging\": \"<22.0dev,>=14.3\",\n","#         \"grpcio\": \"<2.0dev,>=1.51.3\",\n","#         \"jupyter-lsp\": \">=2.0.0\",\n","#         \"google-cloud-storage\": \"<3,>=2.2.1\",\n","#         \"shapely\": \">=2.0.1\",\n","#         \"numpy\": \"<1.25,>=1.21\",\n","#         \"cryptography\": \"<42,>=38.0.0\",\n","#         \"numpy\": \"<1.22.2,>=1.15.0\",\n","#         \"scipy\": \"<1.8.0,>=1.7.3\",\n","#         \"fsspec\": \"==2023.12.2\",\n","#         \"typing-extensions\": \"<4.6.0,>=3.6.6\",\n","#         \"platformdirs\": \"<4,>=2.4\",\n","#         \"numpy\": \"<1.24,>=1.16.0\",\n","#         \"pandas\": \"!=1.4.0,<2.1,>1.1\",\n","#         \"scipy\": \"<1.12,>=1.4.1\",\n","#     }\n","\n","#     for package, version_spec in required_versions.items():\n","#         !pip install \"{package} {version_spec}\"\n","\n","#     # Verify that conflicts are resolved\n","#     working_set = pkg_resources.WorkingSet()\n","#     if not working_set.require(required_versions):\n","#         raise RuntimeError(\"Dependency conflicts could not be resolved.\")\n","\n","#     print(\"Dependency conflicts resolved!\")\n","\n","# if __name__ == \"__main__\":\n","#     resolve_conflicts()\n"]},{"cell_type":"markdown","metadata":{},"source":["**Importing necessary packages**"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T16:47:42.436484Z","iopub.status.busy":"2024-01-27T16:47:42.436128Z","iopub.status.idle":"2024-01-27T16:47:43.627715Z","shell.execute_reply":"2024-01-27T16:47:43.625715Z","shell.execute_reply.started":"2024-01-27T16:47:42.436450Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import os\n","import torch\n","import transformers\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","from peft import prepare_model_for_kbit_training\n","from peft import LoraConfig, get_peft_model\n","from trl import SFTTrainer"]},{"cell_type":"markdown","metadata":{},"source":["**Model: https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.3?text=My+name+is+Clara+and+I+am**"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.629729Z","iopub.status.idle":"2024-01-27T16:47:43.630082Z","shell.execute_reply":"2024-01-27T16:47:43.629937Z","shell.execute_reply.started":"2024-01-27T16:47:43.629921Z"},"trusted":true},"outputs":[],"source":["# Model from Hugging Face hub\n","base_model = \"PY007/TinyLlama-1.1B-Chat-v0.3\"\n","\n","# Fine-tuned model\n","new_model = \"TinyLlama-chat-medaid-model\""]},{"cell_type":"markdown","metadata":{},"source":["**Reading the PDF and storing it as a text file**"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.631566Z","iopub.status.idle":"2024-01-27T16:47:43.631903Z","shell.execute_reply":"2024-01-27T16:47:43.631730Z","shell.execute_reply.started":"2024-01-27T16:47:43.631716Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Generating train split: 53861 examples [00:00, 1110414.60 examples/s]\n"]}],"source":["# READING THE NEW CONVERSATION DATASET WITH ALL THE TEXT FILES (doctor-patient-conversation-large)\n","\n","# Directory containing your text files\n","text_files_directory = r\"G:\\LLM-Model-MedAid-Thesis\\Model-3\\Dataset\"\n","\n","# List to store individual conversation texts\n","conversation_texts = []\n","\n","# Loop through each text file in the directory\n","for filename in os.listdir(text_files_directory):\n","    if filename.endswith(\".txt\"):\n","        file_path = os.path.join(text_files_directory, filename)\n","        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n","            # Use errors=\"ignore\" to skip characters that cannot be decoded\n","            conversation_texts.append(file.read())\n","\n","# Save the combined text to a file\n","output_path = r\"G:\\LLM-Model-MedAid-Thesis\\Model-3\\combined_conversations.txt\"\n","with open(output_path, \"w\", encoding=\"utf-8\") as output_file:\n","    for text in conversation_texts:\n","        output_file.write(text + \"\\n\\n\")\n","\n","dataset = load_dataset(\"text\", data_files=output_path, split=\"train\")\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.632802Z","iopub.status.idle":"2024-01-27T16:47:43.633132Z","shell.execute_reply":"2024-01-27T16:47:43.632980Z","shell.execute_reply.started":"2024-01-27T16:47:43.632965Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['text'],\n","    num_rows: 53861\n","})\n"]}],"source":["print(dataset)"]},{"cell_type":"markdown","metadata":{},"source":["**This code has a configuration for quantization using the *BitsAndBytesConfig class* from the *trl library*. Quantization is a technique used in deep learning to reduce the memory and computational requirements of neural networks.**"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.634612Z","iopub.status.idle":"2024-01-27T16:47:43.634961Z","shell.execute_reply":"2024-01-27T16:47:43.634814Z","shell.execute_reply.started":"2024-01-27T16:47:43.634794Z"},"trusted":true},"outputs":[],"source":["compute_dtype = getattr(torch, \"float16\")\n","\n","quant_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    # bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_compute_dtype=torch.bfloat16,\n","    bnb_4bit_use_double_quant=False,\n",")"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.4\n"]}],"source":["\n","from torch.utils.cpp_extension import CUDA_HOME\n","print(CUDA_HOME) # by default it is set to /usr/local/cuda/"]},{"cell_type":"markdown","metadata":{},"source":["**This code is using the AutoModelForCausalLM class from the transformers library to instantiate a pre-trained causal language model with specific configurations.**"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.636678Z","iopub.status.idle":"2024-01-27T16:47:43.637011Z","shell.execute_reply":"2024-01-27T16:47:43.636868Z","shell.execute_reply.started":"2024-01-27T16:47:43.636853Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cuda  True\n","bin c:\\Python310\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda118.dll\n"]}],"source":["import torch\n","print(\"Cuda \", torch.cuda.is_available())\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    base_model,\n","    quantization_config=quant_config,\n","    # device_map={\"\": 0}\n","    device_map=\"auto\", # automatically figures out how to best use CPU + GPU for loading model\n","    trust_remote_code=True, # prevents running custom model files on your machine\n",")\n","\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n"]},{"cell_type":"markdown","metadata":{},"source":["**This code is using the AutoTokenizer class from the transformers library to instantiate a tokenizer for a pre-trained language model.**"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.638414Z","iopub.status.idle":"2024-01-27T16:47:43.638745Z","shell.execute_reply":"2024-01-27T16:47:43.638600Z","shell.execute_reply.started":"2024-01-27T16:47:43.638584Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["\n","# Create the tokenizer on the chosen device\n","# tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n","tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\""]},{"cell_type":"markdown","metadata":{},"source":["Prepare Model for Training"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["model.train() # model in training mode (dropout modules are activated)\n","\n","# enable gradient check pointing\n","model.gradient_checkpointing_enable()\n","\n","# enable quantized training\n","model = prepare_model_for_kbit_training(model)"]},{"cell_type":"markdown","metadata":{},"source":["**This code is defining a configuration for the LoRA (Local Reparameterization with Attention) model using the LoraConfig class, which appears to be part of the peft library.**"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.639681Z","iopub.status.idle":"2024-01-27T16:47:43.640010Z","shell.execute_reply":"2024-01-27T16:47:43.639868Z","shell.execute_reply.started":"2024-01-27T16:47:43.639853Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable params: 720,896 || all params: 1,100,781,568 || trainable%: 0.0654894686608706\n"]}],"source":["# Create a LoraConfig instance\n","peft_params = LoraConfig(\n","    r=8,\n","    lora_alpha=32,\n","    target_modules=[\"q_proj\"],\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","# LoRA trainable version of model\n","model = get_peft_model(model, peft_params)\n","\n","# trainable parameter count\n","model.print_trainable_parameters()"]},{"cell_type":"markdown","metadata":{},"source":["**Preparing Training Dataset**"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# # create tokenize function\n","# def tokenize_function(examples):\n","#     # extract text\n","#     text = examples[\"example\"]\n","\n","#     #tokenize and truncate text\n","#     tokenizer.truncation_side = \"left\"\n","#     tokenized_inputs = tokenizer(\n","#         text,\n","#         return_tensors=\"np\",\n","#         truncation=True,\n","#         max_length=512\n","#     )\n","\n","#     return tokenized_inputs\n","\n","# # tokenize training and validation datasets\n","# tokenized_data = dataset.map(tokenize_function, batched=True)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# setting pad token\n","tokenizer.pad_token = tokenizer.eos_token\n","# data collator\n","data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)"]},{"cell_type":"markdown","metadata":{},"source":["**This code is defining a set of training parameters using the TrainingArguments class, which is often used in the transformers library for configuring training settings.**"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.641661Z","iopub.status.idle":"2024-01-27T16:47:43.642104Z","shell.execute_reply":"2024-01-27T16:47:43.641905Z","shell.execute_reply.started":"2024-01-27T16:47:43.641884Z"},"trusted":true},"outputs":[],"source":["# hyperparameters\n","lr = 2e-4\n","batch_size = 4\n","num_epochs = 10\n","\n","# Specify the output directory and other training parameters\n","training_params = TrainingArguments(\n","    output_dir=\"./results\",\n","    learning_rate=lr,\n","    per_device_train_batch_size=batch_size,\n","    num_train_epochs=num_epochs,\n","    gradient_accumulation_steps=4,\n","    # save_steps=25,\n","    logging_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    # logging_steps=25,  \n","    weight_decay=0.01,\n","    # load_best_model_at_end=True,\n","    warmup_steps=2,\n","    fp16=True,\n","    optim=\"paged_adamw_8bit\",\n","    # bf16=False,\n","    # max_grad_norm=0.3,\n","    # max_steps=-1,\n","    # warmup_ratio=0.03,\n","    # group_by_length=True,\n","    # lr_scheduler_type=\"constant\",\n","    # report_to=\"tensorboard\"\n",")"]},{"cell_type":"markdown","metadata":{},"source":["**This code is creating an instance of the SFTTrainer class, presumably from the trl library, to facilitate the training of a model using the specified configuration.**\n","\n","**SFTTrainer instance is configured with the** \n","* model, \n","* training dataset, \n","* Peft configuration, \n","* tokenizer,\n","* training arguments. \n","\n","**The specific behavior and training process are determined by the SFTTrainer implementation in the trl library, and the configured parameters influence aspects such as optimization, learning rate, and model architecture during training.**"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.643115Z","iopub.status.idle":"2024-01-27T16:47:43.643436Z","shell.execute_reply":"2024-01-27T16:47:43.643292Z","shell.execute_reply.started":"2024-01-27T16:47:43.643277Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\User\\AppData\\Roaming\\Python\\Python310\\site-packages\\trl\\trainer\\sft_trainer.py:222: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n","  warnings.warn(\n","Map:   0%|          | 0/53861 [00:00<?, ? examples/s]"]},{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 53861/53861 [00:00<00:00, 68700.12 examples/s]\n"]}],"source":["# Create the trainer\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_params,\n","    dataset_text_field=\"text\",\n","    max_seq_length=None,\n","    tokenizer=tokenizer,\n","    args=training_params,\n","    packing=False,\n","    data_collator=data_collator\n",")"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.644647Z","iopub.status.idle":"2024-01-27T16:47:43.644991Z","shell.execute_reply":"2024-01-27T16:47:43.644847Z","shell.execute_reply.started":"2024-01-27T16:47:43.644832Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/33660 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","c:\\Python310\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"," 10%|█         | 3366/33660 [56:06<8:40:38,  1.03s/it] Checkpoint destination directory ./results\\checkpoint-3366 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n","c:\\Python310\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 3.233, 'learning_rate': 0.00018002852219383209, 'epoch': 1.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 20%|██        | 6733/33660 [1:52:12<7:16:51,  1.03it/s] c:\\Python310\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 2.9897, 'learning_rate': 0.0001600332758928041, 'epoch': 2.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 30%|███       | 10099/33660 [2:48:21<7:21:09,  1.12s/it]c:\\Python310\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 2.9014, 'learning_rate': 0.0001400380295917761, 'epoch': 3.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|████      | 13466/33660 [3:44:31<5:21:11,  1.05it/s] c:\\Python310\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 2.9055, 'learning_rate': 0.00012004278329074811, 'epoch': 4.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 16832/33660 [4:40:16<4:29:26,  1.04it/s]c:\\Python310\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 2.8222, 'learning_rate': 0.00010005347911343515, 'epoch': 5.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 60%|██████    | 20199/33660 [5:36:04<3:39:26,  1.02it/s]c:\\Python310\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 2.8081, 'learning_rate': 8.005823281240717e-05, 'epoch': 6.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 70%|███████   | 23565/33660 [6:31:52<2:38:05,  1.06it/s]c:\\Python310\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 2.7862, 'learning_rate': 6.006298651137917e-05, 'epoch': 7.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 80%|████████  | 26932/33660 [7:27:41<1:41:52,  1.10it/s]c:\\Python310\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 2.7927, 'learning_rate': 4.006179808663617e-05, 'epoch': 8.0}\n"]},{"name":"stderr","output_type":"stream","text":[" 90%|█████████ | 30298/33660 [8:23:06<54:58,  1.02it/s]  c:\\Python310\\lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 2.704, 'learning_rate': 2.0072493909323193e-05, 'epoch': 9.0}\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 33660/33660 [9:18:24<00:00,  1.04it/s]  "]},{"name":"stdout","output_type":"stream","text":["{'loss': 2.7049, 'learning_rate': 1.010161031552677e-07, 'epoch': 10.0}\n","{'train_runtime': 33504.5782, 'train_samples_per_second': 16.076, 'train_steps_per_second': 1.005, 'train_loss': 2.864780009423277, 'epoch': 10.0}\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 33660/33660 [9:18:24<00:00,  1.00it/s]\n"]}],"source":["# train model\n","model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n","trainer.train()\n","\n","# renable warnings\n","model.config.use_cache = True"]},{"cell_type":"markdown","metadata":{},"source":["**The code you provided is saving the trained model and its associated tokenizer to a specified directory using the save_pretrained method.**"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.646365Z","iopub.status.idle":"2024-01-27T16:47:43.646701Z","shell.execute_reply":"2024-01-27T16:47:43.646547Z","shell.execute_reply.started":"2024-01-27T16:47:43.646531Z"},"trusted":true},"outputs":[{"data":{"text/plain":["('preTrained_model\\\\tokenizer_config.json',\n"," 'preTrained_model\\\\special_tokens_map.json',\n"," 'preTrained_model\\\\tokenizer.model',\n"," 'preTrained_model\\\\added_tokens.json',\n"," 'preTrained_model\\\\tokenizer.json')"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["model_path = \"preTrained_model\"\n","\n","trainer.save_model(model_path)\n","tokenizer.save_pretrained(model_path)\n","\n","# trainer.model.save_pretrained(new_model)\n","# trainer.tokenizer.save_pretrained(new_model)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["# Import the pretrained model\n","model = AutoModelForCausalLM.from_pretrained(model_path)\n","tokenizer = AutoTokenizer.from_pretrained(model_path)"]},{"cell_type":"markdown","metadata":{},"source":["**This code creates a simple conversational loop that simulates a doctor assistant interaction. It uses a fine-tuned language model (assumed to be a text generation model) and a tokenizer to generate responses based on user input. The conversation is logged in a text file, and the loop continues until the user provides an exit signal.**"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.647706Z","iopub.status.idle":"2024-01-27T16:47:43.648052Z","shell.execute_reply":"2024-01-27T16:47:43.647908Z","shell.execute_reply.started":"2024-01-27T16:47:43.647892Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Both `max_new_tokens` (=32) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"name":"stdout","output_type":"stream","text":["<s>[Prompt] Be a doctor assistant. And keep questioning one by one to extract symptoms and history of the patient. Don't give advice or ask anything else. Just extract symptoms or history by questioning one question at a time. \n"," [User response] I have chest pain. \n"," [/Response] [/User]\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"name":"stderr","output_type":"stream","text":["Both `max_new_tokens` (=32) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"name":"stdout","output_type":"stream","text":["<s>[Prompt] Be a doctor assistant. And keep questioning one by one to extract symptoms and history of the patient. Don't give advice or ask anything else. Just extract symptoms or history by questioning one question at a time. \n"," [User response] I have chest pain \n"," [/Response] [User response] [User response] [User response] [User response] [User response] [User response] [User response] [User response]\n"]},{"name":"stderr","output_type":"stream","text":["Both `max_new_tokens` (=32) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"name":"stdout","output_type":"stream","text":["<s>[Prompt] Be a doctor assistant. And keep questioning one by one to extract symptoms and history of the patient. Don't give advice or ask anything else. Just extract symptoms or history by questioning one question at a time. \n"," [User response] I am 30 years old \n"," [/Response]\n","\n","[Prompt] Do you have any allergies? [/Prompt]\n","[User response] [/User response]\n","\n","[\n"]},{"name":"stderr","output_type":"stream","text":["Both `max_new_tokens` (=32) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"name":"stdout","output_type":"stream","text":["<s>[Prompt] Be a doctor assistant. And keep questioning one by one to extract symptoms and history of the patient. Don't give advice or ask anything else. Just extract symptoms or history by questioning one question at a time. \n"," [User response] yes \n"," [/Response]\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"name":"stderr","output_type":"stream","text":["Both `max_new_tokens` (=32) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"name":"stdout","output_type":"stream","text":["<s>[Prompt] Be a doctor assistant. And keep questioning one by one to extract symptoms and history of the patient. Don't give advice or ask anything else. Just extract symptoms or history by questioning one question at a time. \n"," [User response] i have pain in my chest \n"," [/Response] \n","[User response] [Tone] [Phrase] [Phrase] [Phrase] [Phrase] [P\n","Ending the conversation.\n"]}],"source":["# logging.set_verbosity(logging.CRITICAL)\n","\n","# Initial prompt\n","prompt = \"Be a doctor assistant. And keep questioning one by one to extract symptoms and history of the patient. Don't give advice or ask anything else. Just extract symptoms or history by questioning one question at a time.\"\n","\n","# Create a pipeline for text generation using the fine-tuned model and tokenizer\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n","\n","# File to save the conversation\n","output_file_path = \"G:\\LLM-Model-MedAid-Thesis\\Model-3\\conversation_log.txt\"  # Update 'your_folder' with the desired folder in your Drive\n","\n","# Function to ask a question and get the user's response\n","def ask_question_and_log(prompt, user_response, file_path):\n","    # Ask the question\n","    result = pipe(f\"<s>[Prompt] {prompt} \\n [User response] {user_response} \\n [/Response]\")\n","\n","    # Get the generated text (question)\n","    generated_text = result[0]['generated_text']\n","\n","    # Print and save the generated text (question)\n","    print(generated_text)\n","    with open(file_path, \"a\", encoding=\"utf-8\") as output_file:\n","        output_file.write(f\"[Model] {generated_text}\\n\\n\")\n","\n","    # Simulate user answering the question\n","    user_response = input(\"Your response: \")  # User provides input\n","\n","    return user_response\n","\n","# Initial user response\n","user_response = \"I have chest pain.\"\n","\n","# Ask a question based on the user's response and log the conversation\n","with open(output_file_path, \"a\", encoding=\"utf-8\") as output_file:\n","    output_file.write(\"\\nConversation started.\\n\\n\")\n","\n","# Loop to continue the conversation\n","while True:\n","    user_response = ask_question_and_log(prompt, user_response, output_file_path)\n","\n","    # Check for an exit condition (e.g., user response indicating the end of the conversation)\n","    if \"exit\" in user_response.lower():\n","        print(\"Ending the conversation.\")\n","\n","        # Save the conversation to a file\n","        with open(output_file_path, \"a\", encoding=\"utf-8\") as output_file:\n","            output_file.write(\"\\nConversation ended by user. ---------------- \\n\\n\")\n","\n","        break\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4256693,"sourceId":7332627,"sourceType":"datasetVersion"},{"datasetId":4354465,"sourceId":7480410,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"tinyLlamaMedAid","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.1"}},"nbformat":4,"nbformat_minor":4}
