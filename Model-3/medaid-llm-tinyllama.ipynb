{"cells":[{"cell_type":"markdown","metadata":{},"source":["INSTALL PACKAGES"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T16:47:12.027287Z","iopub.status.busy":"2024-01-27T16:47:12.026981Z","iopub.status.idle":"2024-01-27T16:47:42.426119Z","shell.execute_reply":"2024-01-27T16:47:42.425125Z","shell.execute_reply.started":"2024-01-27T16:47:12.027264Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: accelerate in c:\\users\\user\\anaconda3\\lib\\site-packages (0.29.2)\n","Requirement already satisfied: peft in c:\\users\\user\\anaconda3\\lib\\site-packages (0.10.0)\n","Requirement already satisfied: transformers in c:\\users\\user\\anaconda3\\lib\\site-packages (4.39.3)\n","Requirement already satisfied: pdfplumber in c:\\users\\user\\anaconda3\\lib\\site-packages (0.11.0)\n","Requirement already satisfied: datasets==2.16.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (2.16.1)\n","Requirement already satisfied: trl in c:\\users\\user\\anaconda3\\lib\\site-packages (0.8.3)\n","Requirement already satisfied: multiprocess in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets==2.16.1) (0.70.15)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets==2.16.1) (0.3.7)\n","Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets==2.16.1) (1.4.4)\n","Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets==2.16.1) (6.0)\n","Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets==2.16.1) (21.3)\n","Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets==2.16.1) (12.0.0)\n","Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets==2.16.1) (4.64.1)\n","Requirement already satisfied: aiohttp in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets==2.16.1) (3.8.3)\n","Requirement already satisfied: huggingface-hub>=0.19.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets==2.16.1) (0.20.2)\n","Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets==2.16.1) (1.24.3)\n","Requirement already satisfied: xxhash in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets==2.16.1) (3.4.1)\n","Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets==2.16.1) (3.6.0)\n","Requirement already satisfied: requests>=2.19.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets==2.16.1) (2.28.1)\n","Requirement already satisfied: pyarrow-hotfix in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets==2.16.1) (0.6)\n","Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from datasets==2.16.1) (2023.10.0)\n","Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from accelerate) (0.4.2)\n","Requirement already satisfied: torch>=1.10.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from accelerate) (2.1.0)\n","Requirement already satisfied: psutil in c:\\users\\user\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n","Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.15.0)\n","Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pdfplumber) (4.26.0)\n","Requirement already satisfied: Pillow>=9.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pdfplumber) (10.0.1)\n","Requirement already satisfied: pdfminer.six==20231228 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pdfplumber) (20231228)\n","Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (37.0.1)\n","Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (2.0.4)\n","Requirement already satisfied: tyro>=0.5.11 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from trl) (0.6.3)\n","Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.16.1) (6.0.2)\n","Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.16.1) (21.4.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.16.1) (4.0.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.16.1) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.16.1) (1.2.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp->datasets==2.16.1) (1.8.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets==2.16.1) (4.3.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from packaging->datasets==2.16.1) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets==2.16.1) (2024.2.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets==2.16.1) (1.26.11)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets==2.16.1) (3.3)\n","Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (2.8.4)\n","Requirement already satisfied: sympy in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.10.1)\n","Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (2.11.3)\n","Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets==2.16.1) (0.4.5)\n","Requirement already satisfied: rich>=11.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tyro>=0.5.11->trl) (13.7.0)\n","Requirement already satisfied: docstring-parser>=0.14.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tyro>=0.5.11->trl) (0.15)\n","Requirement already satisfied: shtab>=1.5.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from tyro>=0.5.11->trl) (1.6.5)\n","Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas->datasets==2.16.1) (2022.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas->datasets==2.16.1) (2.8.2)\n","Requirement already satisfied: cffi>=1.12 in c:\\users\\user\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.15.1)\n","Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets==2.16.1) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.17.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.0.1)\n","Requirement already satisfied: mpmath>=0.19 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.2.1)\n","Requirement already satisfied: pycparser in c:\\users\\user\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.21)\n","Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n","Note: you may need to restart the kernel to use updated packages.\n","Looking in indexes: https://pypi.org/simple, https://jllllll.github.io/bitsandbytes-windows-webui\n","Requirement already satisfied: bitsandbytes in c:\\users\\user\\anaconda3\\lib\\site-packages (0.42.0)\n","Requirement already satisfied: scipy in c:\\users\\user\\anaconda3\\lib\\site-packages (from bitsandbytes) (1.10.1)\n","Requirement already satisfied: numpy<1.27.0,>=1.19.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scipy->bitsandbytes) (1.24.3)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["# !pip install --upgrade accelerate peft bitsandbytes transformers pdfplumber datasets==2.16.1 trl==0.7.9 --target=/kaggle/working/packages\n","%pip install --upgrade accelerate peft transformers pdfplumber datasets==2.16.1 trl\n","%pip install bitsandbytes --prefer-binary --extra-index-url=https://jllllll.github.io/bitsandbytes-windows-webui\n"]},{"cell_type":"markdown","metadata":{},"source":["**To resolve some issues while installing packages**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T16:47:42.428523Z","iopub.status.busy":"2024-01-27T16:47:42.427733Z","iopub.status.idle":"2024-01-27T16:47:42.434850Z","shell.execute_reply":"2024-01-27T16:47:42.433821Z","shell.execute_reply.started":"2024-01-27T16:47:42.428486Z"},"trusted":true},"outputs":[],"source":["# import pkg_resources\n","\n","# def resolve_conflicts():\n","#     \"\"\"\n","#     Resolves dependency conflicts by creating a new environment and installing compatible versions.\n","#     \"\"\"\n","\n","#     # Create a new virtual environment (replace \"myvenv\" with your desired name)\n","#     !virtualenv myvenv\n","#     !source myvenv/bin/activate\n","\n","#     # Install compatible versions of conflicting packages\n","#     required_versions = {\n","#         \"cupy-cuda11x\": \">=12.0.0\",\n","#         \"dill\": \"<0.3.2,>=0.3.1.1\",\n","#         \"numpy\": \"<1.25.0,>=1.14.3\",\n","#         \"pyarrow\": \"<10.0.0,>=3.0.0\",\n","#         \"jupyter-server\": \"~=1.16\",\n","#         \"jupyterlab\": \"~=3.4\",\n","#         \"urllib3\": \"<2.1,>=1.25.4\",  # Ensure python_version >= \"3.10\"\n","#         \"pandas\": \"<1.6.0dev0,>=1.3\",\n","#         \"protobuf\": \"<5,>=4.21\",\n","#         \"dask\": \"==2023.7.1\",\n","#         \"distributed\": \"==2023.7.1\",\n","#         \"fsspec\": \"==2023.6.0\",\n","#         \"urllib3\": \"<2.0\",\n","#         \"google-api-core[grpc]\": \"<2.0.0dev,>=1.22.2\",\n","#         \"packaging\": \"<22.0dev,>=14.3\",\n","#         \"grpcio\": \"<2.0dev,>=1.51.3\",\n","#         \"jupyter-lsp\": \">=2.0.0\",\n","#         \"google-cloud-storage\": \"<3,>=2.2.1\",\n","#         \"shapely\": \">=2.0.1\",\n","#         \"numpy\": \"<1.25,>=1.21\",\n","#         \"cryptography\": \"<42,>=38.0.0\",\n","#         \"numpy\": \"<1.22.2,>=1.15.0\",\n","#         \"scipy\": \"<1.8.0,>=1.7.3\",\n","#         \"fsspec\": \"==2023.12.2\",\n","#         \"typing-extensions\": \"<4.6.0,>=3.6.6\",\n","#         \"platformdirs\": \"<4,>=2.4\",\n","#         \"numpy\": \"<1.24,>=1.16.0\",\n","#         \"pandas\": \"!=1.4.0,<2.1,>1.1\",\n","#         \"scipy\": \"<1.12,>=1.4.1\",\n","#     }\n","\n","#     for package, version_spec in required_versions.items():\n","#         !pip install \"{package} {version_spec}\"\n","\n","#     # Verify that conflicts are resolved\n","#     working_set = pkg_resources.WorkingSet()\n","#     if not working_set.require(required_versions):\n","#         raise RuntimeError(\"Dependency conflicts could not be resolved.\")\n","\n","#     print(\"Dependency conflicts resolved!\")\n","\n","# if __name__ == \"__main__\":\n","#     resolve_conflicts()\n"]},{"cell_type":"markdown","metadata":{},"source":["**Importing necessary packages**"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T16:47:42.436484Z","iopub.status.busy":"2024-01-27T16:47:42.436128Z","iopub.status.idle":"2024-01-27T16:47:43.627715Z","shell.execute_reply":"2024-01-27T16:47:43.625715Z","shell.execute_reply.started":"2024-01-27T16:47:42.436450Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import os\n","import torch\n","import transformers\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","from peft import prepare_model_for_kbit_training\n","from peft import LoraConfig, get_peft_model\n","from trl import SFTTrainer"]},{"cell_type":"markdown","metadata":{},"source":["**Model: https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.3?text=My+name+is+Clara+and+I+am**"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.629729Z","iopub.status.idle":"2024-01-27T16:47:43.630082Z","shell.execute_reply":"2024-01-27T16:47:43.629937Z","shell.execute_reply.started":"2024-01-27T16:47:43.629921Z"},"trusted":true},"outputs":[],"source":["# Model from Hugging Face hub\n","base_model = \"PY007/TinyLlama-1.1B-Chat-v0.3\"\n","\n","# Fine-tuned model\n","new_model = \"TinyLlama-chat-medaid-model\""]},{"cell_type":"markdown","metadata":{},"source":["**Reading the PDF and storing it as a text file**"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.631566Z","iopub.status.idle":"2024-01-27T16:47:43.631903Z","shell.execute_reply":"2024-01-27T16:47:43.631730Z","shell.execute_reply.started":"2024-01-27T16:47:43.631716Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Generating train split: 53861 examples [00:00, 1282424.91 examples/s]\n"]}],"source":["# READING THE NEW CONVERSATION DATASET WITH ALL THE TEXT FILES (doctor-patient-conversation-large)\n","\n","# Directory containing your text files\n","text_files_directory = r\"G:\\LLM-Model-MedAid-Thesis\\Model-3\\Dataset\"\n","\n","# List to store individual conversation texts\n","conversation_texts = []\n","\n","# Loop through each text file in the directory\n","for filename in os.listdir(text_files_directory):\n","    if filename.endswith(\".txt\"):\n","        file_path = os.path.join(text_files_directory, filename)\n","        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n","            # Use errors=\"ignore\" to skip characters that cannot be decoded\n","            conversation_texts.append(file.read())\n","\n","# Save the combined text to a file\n","output_path = r\"G:\\LLM-Model-MedAid-Thesis\\Model-3\\combined_conversations.txt\"\n","with open(output_path, \"w\", encoding=\"utf-8\") as output_file:\n","    for text in conversation_texts:\n","        output_file.write(text + \"\\n\\n\")\n","\n","dataset = load_dataset(\"text\", data_files=output_path, split=\"train\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.632802Z","iopub.status.idle":"2024-01-27T16:47:43.633132Z","shell.execute_reply":"2024-01-27T16:47:43.632980Z","shell.execute_reply.started":"2024-01-27T16:47:43.632965Z"},"trusted":true},"outputs":[],"source":["print(dataset)"]},{"cell_type":"markdown","metadata":{},"source":["**This code has a configuration for quantization using the *BitsAndBytesConfig class* from the *trl library*. Quantization is a technique used in deep learning to reduce the memory and computational requirements of neural networks.**"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.634612Z","iopub.status.idle":"2024-01-27T16:47:43.634961Z","shell.execute_reply":"2024-01-27T16:47:43.634814Z","shell.execute_reply.started":"2024-01-27T16:47:43.634794Z"},"trusted":true},"outputs":[],"source":["compute_dtype = getattr(torch, \"float16\")\n","\n","quant_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    # bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_compute_dtype=torch.bfloat16,\n","    bnb_4bit_use_double_quant=False,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","from torch.utils.cpp_extension import CUDA_HOME\n","print(CUDA_HOME) # by default it is set to /usr/local/cuda/"]},{"cell_type":"markdown","metadata":{},"source":["**This code is using the AutoModelForCausalLM class from the transformers library to instantiate a pre-trained causal language model with specific configurations.**"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.636678Z","iopub.status.idle":"2024-01-27T16:47:43.637011Z","shell.execute_reply":"2024-01-27T16:47:43.636868Z","shell.execute_reply.started":"2024-01-27T16:47:43.636853Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cuda  True\n"]}],"source":["import torch\n","print(\"Cuda \", torch.cuda.is_available())\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    base_model,\n","    quantization_config=quant_config,\n","    # device_map={\"\": 0}\n","    device_map=\"auto\", # automatically figures out how to best use CPU + GPU for loading model\n","    trust_remote_code=True, # prevents running custom model files on your machine\n",")\n","\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n"]},{"cell_type":"markdown","metadata":{},"source":["**This code is using the AutoTokenizer class from the transformers library to instantiate a tokenizer for a pre-trained language model.**"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.638414Z","iopub.status.idle":"2024-01-27T16:47:43.638745Z","shell.execute_reply":"2024-01-27T16:47:43.638600Z","shell.execute_reply.started":"2024-01-27T16:47:43.638584Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["\n","# Create the tokenizer on the chosen device\n","# tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n","tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\""]},{"cell_type":"markdown","metadata":{},"source":["Prepare Model for Training"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["model.train() # model in training mode (dropout modules are activated)\n","\n","# enable gradient check pointing\n","model.gradient_checkpointing_enable()\n","\n","# enable quantized training\n","model = prepare_model_for_kbit_training(model)"]},{"cell_type":"markdown","metadata":{},"source":["**This code is defining a configuration for the LoRA (Local Reparameterization with Attention) model using the LoraConfig class, which appears to be part of the peft library.**"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.639681Z","iopub.status.idle":"2024-01-27T16:47:43.640010Z","shell.execute_reply":"2024-01-27T16:47:43.639868Z","shell.execute_reply.started":"2024-01-27T16:47:43.639853Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable params: 720,896 || all params: 1,100,781,568 || trainable%: 0.0654894686608706\n"]}],"source":["# Create a LoraConfig instance\n","peft_params = LoraConfig(\n","    r=8,\n","    lora_alpha=32,\n","    target_modules=[\"q_proj\"],\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","# LoRA trainable version of model\n","model = get_peft_model(model, peft_params)\n","\n","# trainable parameter count\n","model.print_trainable_parameters()"]},{"cell_type":"markdown","metadata":{},"source":["**Preparing Training Dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # create tokenize function\n","# def tokenize_function(examples):\n","#     # extract text\n","#     text = examples[\"example\"]\n","\n","#     #tokenize and truncate text\n","#     tokenizer.truncation_side = \"left\"\n","#     tokenized_inputs = tokenizer(\n","#         text,\n","#         return_tensors=\"np\",\n","#         truncation=True,\n","#         max_length=512\n","#     )\n","\n","#     return tokenized_inputs\n","\n","# # tokenize training and validation datasets\n","# tokenized_data = dataset.map(tokenize_function, batched=True)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# setting pad token\n","tokenizer.pad_token = tokenizer.eos_token\n","# data collator\n","data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)"]},{"cell_type":"markdown","metadata":{},"source":["**This code is defining a set of training parameters using the TrainingArguments class, which is often used in the transformers library for configuring training settings.**"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.641661Z","iopub.status.idle":"2024-01-27T16:47:43.642104Z","shell.execute_reply":"2024-01-27T16:47:43.641905Z","shell.execute_reply.started":"2024-01-27T16:47:43.641884Z"},"trusted":true},"outputs":[],"source":["# hyperparameters\n","lr = 2e-4\n","batch_size = 4\n","num_epochs = 10\n","\n","# Specify the output directory and other training parameters\n","training_params = TrainingArguments(\n","    output_dir=\"./results\",\n","    learning_rate=lr,\n","    per_device_train_batch_size=batch_size,\n","    num_train_epochs=num_epochs,\n","    gradient_accumulation_steps=4,\n","    # save_steps=25,\n","    logging_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    # logging_steps=25,  \n","    weight_decay=0.01,\n","    # load_best_model_at_end=True,\n","    warmup_steps=2,\n","    fp16=True,\n","    optim=\"paged_adamw_8bit\",\n","    # bf16=False,\n","    # max_grad_norm=0.3,\n","    # max_steps=-1,\n","    # warmup_ratio=0.03,\n","    # group_by_length=True,\n","    # lr_scheduler_type=\"constant\",\n","    # report_to=\"tensorboard\"\n",")"]},{"cell_type":"markdown","metadata":{},"source":["**This code is creating an instance of the SFTTrainer class, presumably from the trl library, to facilitate the training of a model using the specified configuration.**\n","\n","**SFTTrainer instance is configured with the** \n","* model, \n","* training dataset, \n","* Peft configuration, \n","* tokenizer,\n","* training arguments. \n","\n","**The specific behavior and training process are determined by the SFTTrainer implementation in the trl library, and the configured parameters influence aspects such as optimization, learning rate, and model architecture during training.**"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.643115Z","iopub.status.idle":"2024-01-27T16:47:43.643436Z","shell.execute_reply":"2024-01-27T16:47:43.643292Z","shell.execute_reply.started":"2024-01-27T16:47:43.643277Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\User\\AppData\\Roaming\\Python\\Python310\\site-packages\\trl\\trainer\\sft_trainer.py:222: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n","  warnings.warn(\n","Map: 100%|██████████| 53861/53861 [00:00<00:00, 89470.38 examples/s]\n"]}],"source":["# Create the trainer\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_params,\n","    dataset_text_field=\"text\",\n","    max_seq_length=None,\n","    tokenizer=tokenizer,\n","    args=training_params,\n","    packing=False,\n","    data_collator=data_collator\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.644647Z","iopub.status.idle":"2024-01-27T16:47:43.644991Z","shell.execute_reply":"2024-01-27T16:47:43.644847Z","shell.execute_reply.started":"2024-01-27T16:47:43.644832Z"},"trusted":true},"outputs":[],"source":["# train model\n","model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n","# trainer.train()\n","\n","# renable warnings\n","model.config.use_cache = True"]},{"cell_type":"markdown","metadata":{},"source":["**The code you provided is saving the trained model and its associated tokenizer to a specified directory using the save_pretrained method.**"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["model_path = \"preTrained_model\""]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.646365Z","iopub.status.idle":"2024-01-27T16:47:43.646701Z","shell.execute_reply":"2024-01-27T16:47:43.646547Z","shell.execute_reply.started":"2024-01-27T16:47:43.646531Z"},"trusted":true},"outputs":[{"data":{"text/plain":["('preTrained_model\\\\tokenizer_config.json',\n"," 'preTrained_model\\\\special_tokens_map.json',\n"," 'preTrained_model\\\\tokenizer.model',\n"," 'preTrained_model\\\\added_tokens.json',\n"," 'preTrained_model\\\\tokenizer.json')"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["trainer.save_model(model_path)\n","tokenizer.save_pretrained(model_path)\n","\n","# trainer.model.save_pretrained(new_model)\n","# trainer.tokenizer.save_pretrained(new_model)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["# Import the pretrained model\n","model = AutoModelForCausalLM.from_pretrained(model_path)\n","tokenizer = AutoTokenizer.from_pretrained(model_path)"]},{"cell_type":"markdown","metadata":{},"source":["**This code creates a simple conversational loop that simulates a doctor assistant interaction. It uses a fine-tuned language model (assumed to be a text generation model) and a tokenizer to generate responses based on user input. The conversation is logged in a text file, and the loop continues until the user provides an exit signal.**"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.647706Z","iopub.status.idle":"2024-01-27T16:47:43.648052Z","shell.execute_reply":"2024-01-27T16:47:43.647908Z","shell.execute_reply.started":"2024-01-27T16:47:43.647892Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Both `max_new_tokens` (=32) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"name":"stdout","output_type":"stream","text":["<s>[Prompt] Be a doctor assistant. And keep questioning one by one to extract symptoms and history of the patient. Don't give advice or ask anything else. Just extract symptoms or history by questioning one question at a time. \n"," [User response] \n"," I have chest pain. \n"," [/User Response] \n","\n","[Prompt] Now, ask me about my symptoms. \n"," [User response] \n"," I have chest pain and dizz\n","Ending the conversation.\n"]}],"source":["# logging.set_verbosity(logging.CRITICAL)\n","\n","# Initial prompt\n","prompt = \"Be a doctor assistant. And keep questioning one by one to extract symptoms and history of the patient. Don't give advice or ask anything else. Just extract symptoms or history by questioning one question at a time.\"\n","\n","# Create a pipeline for text generation using the fine-tuned model and tokenizer\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n","\n","# File to save the conversation\n","output_file_path = \"G:\\LLM-Model-MedAid-Thesis\\Model-3\\conversation_log.txt\"  # Update 'your_folder' with the desired folder in your Drive\n","\n","# Function to ask a question and get the user's response\n","def ask_question_and_log(prompt, user_response, file_path):\n","    # Ask the question\n","    result = pipe(f\"<s>[Prompt] {prompt} \\n [User response] \\n {user_response} \\n [/User Response]\")\n","\n","    # Get the generated text (question)\n","    generated_text = result[0]['generated_text']\n","\n","    # Print and save the generated text (question)\n","    print(generated_text)\n","    with open(file_path, \"a\", encoding=\"utf-8\") as output_file:\n","        output_file.write(f\"[Model] {generated_text}\\n\\n\")\n","\n","    # Simulate user answering the question\n","    user_response = input(\"Your response: \")  # User provides input\n","\n","    return user_response\n","\n","# Initial user response\n","user_response = \"I have chest pain.\"\n","\n","# Ask a question based on the user's response and log the conversation\n","with open(output_file_path, \"a\", encoding=\"utf-8\") as output_file:\n","    output_file.write(\"\\nConversation started.\\n\\n\")\n","\n","# Loop to continue the conversation\n","while True:\n","    user_response = ask_question_and_log(prompt, user_response, output_file_path)\n","\n","    # Check for an exit condition (e.g., user response indicating the end of the conversation)\n","    if \"exit\" in user_response.lower():\n","        print(\"Ending the conversation.\")\n","\n","        # Save the conversation to a file\n","        with open(output_file_path, \"a\", encoding=\"utf-8\") as output_file:\n","            output_file.write(\"\\nConversation ended by user. ---------------- \\n\\n\")\n","\n","        break\n"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["As a doctor assistant, I need to understand why you've come today. Please describe your symptoms or reasons for the visit.\n","User response: \"I have chest pain\"\n","Model response: \"<s><Prompt> Please provide additional details about your symptoms or concerns. [User response] \"I have chest pain\" [/User Response] \"I have dizziness\" [/Prompt]\n","I'm not sure what prompt to use for this prompt. I'm not sure what\"\n","Ending the conversation.\n"]}],"source":["# Create a pipeline for text generation\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, torch_dtype=torch.float16, device_map=\"auto\")\n","\n","# File to save the conversation\n","output_file_path = \"G:\\LLM-Model-MedAid-Thesis\\Model-3\\conversation_log.txt\"  # Update 'your_folder' with the desired folder in your Drive\n","\n","# Function to format the prompt\n","def format_prompt(prompt, user_response):\n","    return f\"### Human: {user_response} ### Assistant: {prompt}\"\n","\n","# Function to start the conversation and log it\n","def start_conversation_and_log(file_path):\n","    # Start conversation by asking why the user has come today\n","    initial_prompt = \"As a doctor assistant, I need to understand why you've come today. Please describe your symptoms or reasons for the visit.\"\n","    print(initial_prompt)\n","    with open(file_path, \"a\", encoding=\"utf-8\") as output_file:\n","        output_file.write(f\"[System] {initial_prompt}\\n\\n\")\n","\n","# # Function to ask a question and get the user's response\n","# def ask_question_and_log(user_response, file_path):\n","#     # Ask the question\n","#     result = pipe(f\"<s>[Prompt] What else would you like to share about your symptoms or concerns? \\n [User response] \\\" {user_response} \\\" [/User Response]\")\n","\n","#     # Get the generated text (question)\n","#     generated_text = result[0]['generated_text']\n","\n","#     # Print and save the generated text (question)\n","#     print(\"Response: \", generated_text)\n","#     with open(file_path, \"a\", encoding=\"utf-8\") as output_file:\n","#         output_file.write(f\"[Model] {generated_text}\\n\\n\")\n","\n","#     # Simulate user answering the question\n","#     user_response = input(\"Your response: \")  # User provides input\n","\n","#     return user_response\n","\n","\n","# Function to ask a question and get the user's response\n","def ask_question_and_log(user_response, file_path):\n","    # Ask the question\n","    result = pipe(f\"<s><Prompt> Please provide additional details about your symptoms or concerns. [User response] \\\"{user_response}\\\" [/User Response]\")\n","\n","    # Get the generated text (question)\n","    generated_text = result[0]['generated_text']\n","\n","    # Extract the follow-up question from the generated text\n","    follow_up_question = generated_text.strip()\n","\n","    # Print and save the follow-up question\n","    print(f\"User response: \\\"{user_response}\\\"\")\n","    print(f\"Model response: \\\"{follow_up_question}\\\"\")\n","    with open(file_path, \"a\", encoding=\"utf-8\") as output_file:\n","        output_file.write(f\"[User response] \\\"{user_response}\\\" [/User Response]\\n\")\n","        output_file.write(f\"[Model response] \\\"{follow_up_question}\\\" [/Model Response]\\n\")\n","\n","    # Simulate user answering the question\n","    user_response = input(\"Your response: \")  # User provides input\n","\n","    return user_response\n","\n","\n","\n","\n","\n","# Start the conversation\n","start_conversation_and_log(output_file_path)\n","\n","# Ask questions based on the user's responses and log the conversation\n","with open(output_file_path, \"a\", encoding=\"utf-8\") as output_file:\n","    output_file.write(\"\\nConversation started.\\n\\n\")\n","\n","# Initial user response\n","user_response = input(\"Your response: \")  # User provides input\n","\n","# Loop to continue the conversation\n","while True:\n","    user_response = ask_question_and_log(user_response, output_file_path)\n","\n","    # Check for an exit condition (e.g., user response indicating the end of the conversation)\n","    if \"exit\" in user_response.lower():\n","        print(\"Ending the conversation.\")\n","\n","        # Save the conversation to a file\n","        with open(output_file_path, \"a\", encoding=\"utf-8\") as output_file:\n","            output_file.write(\"\\nConversation ended by user. ---------------- \\n\\n\")\n","\n","        break\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["As a doctor assistant, I need to understand why you've come today. Please describe your symptoms or reasons for the visit.\n","Model response: \"Great! Then please fill out your insurance\"\n","Ending the conversation.\n"]}],"source":["import transformers\n","\n","# Create a pipeline for text generation\n","pipe = transformers.pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n","\n","# File to save the conversation\n","output_file_path = \"G:\\LLM-Model-MedAid-Thesis\\Model-3\\conversation_log.txt\"  # Update 'your_folder' with the desired folder in your Drive\n","\n","# Function to format the prompt\n","def format_prompt(prompt, user_response):\n","    return f\"### Human: {user_response} ### Assistant: {prompt}\"\n","\n","# Function to start the conversation and log it\n","def start_conversation_and_log(file_path):\n","    # Start conversation by asking why the user has come today\n","    initial_prompt = \"As a doctor assistant, I need to understand why you've come today. Please describe your symptoms or reasons for the visit.\"\n","    print(initial_prompt)\n","    with open(file_path, \"a\", encoding=\"utf-8\") as output_file:\n","        output_file.write(f\"[System] {initial_prompt}\\n\\n\")\n","\n","# Function to ask a question and get the user's response\n","def ask_question_and_log(user_response, file_path):\n","    # Format the prompt\n","    formatted_prompt = format_prompt(\"Please provide additional details about your symptoms or concerns.\", user_response)\n","\n","    # Generate response from the model\n","    sequences = pipe(\n","        formatted_prompt,\n","        do_sample=True,\n","        top_k=50,\n","        top_p=0.7,\n","        num_return_sequences=1,\n","        repetition_penalty=1.1,\n","        max_new_tokens=500,\n","    )\n","\n","    # Extract the generated text (follow-up question)\n","    follow_up_question = sequences[0]['generated_text'].split(\"### Assistant: \")[-1]\n","\n","    # Print and save the follow-up question\n","    print(f\"Model response: \\\"{follow_up_question}\\\"\")\n","    with open(file_path, \"a\", encoding=\"utf-8\") as output_file:\n","        output_file.write(f\"[Model response] \\\"{follow_up_question}\\\" [/Model Response]\\n\")\n","\n","    # Extract the follow-up question from the model response\n","    extracted_question = follow_up_question.strip().split(\" \")[0]\n","\n","    # Return the extracted question\n","    return extracted_question\n","\n","# Start the conversation\n","start_conversation_and_log(output_file_path)\n","\n","# Ask questions based on the user's responses and log the conversation\n","with open(output_file_path, \"a\", encoding=\"utf-8\") as output_file:\n","    output_file.write(\"\\nConversation started.\\n\\n\")\n","\n","# Initial user response\n","user_response = input(\"Your response: \")  # User provides input\n","\n","# Loop to continue the conversation\n","while True:\n","    follow_up_question = ask_question_and_log(user_response, output_file_path)\n","\n","    # Simulate user answering the question\n","    user_response = input(f\"Your response to \\\"{follow_up_question}\\\": \")  # User provides input\n","\n","    # Check for an exit condition (e.g., user response indicating the end of the conversation)\n","    if \"exit\" in user_response.lower():\n","        print(\"Ending the conversation.\")\n","\n","        # Save the conversation to a file\n","        with open(output_file_path, \"a\", encoding=\"utf-8\") as output_file:\n","            output_file.write(\"\\nConversation ended by user. ---------------- \\n\\n\")\n","\n","        break"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4256693,"sourceId":7332627,"sourceType":"datasetVersion"},{"datasetId":4354465,"sourceId":7480410,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"tinyLlamaMedAid","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.1"}},"nbformat":4,"nbformat_minor":4}
