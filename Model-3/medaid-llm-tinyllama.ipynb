{"cells":[{"cell_type":"markdown","metadata":{},"source":["INSTALL PACKAGES"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T16:47:12.027287Z","iopub.status.busy":"2024-01-27T16:47:12.026981Z","iopub.status.idle":"2024-01-27T16:47:42.426119Z","shell.execute_reply":"2024-01-27T16:47:42.425125Z","shell.execute_reply.started":"2024-01-27T16:47:12.027264Z"},"trusted":true},"outputs":[],"source":["# !pip install --upgrade accelerate peft bitsandbytes transformers pdfplumber datasets==2.16.1 trl==0.7.9 --target=/kaggle/working/packages\n","%pip install --upgrade accelerate peft transformers pdfplumber datasets==2.16.1 trl\n","%pip install bitsandbytes --prefer-binary --extra-index-url=https://jllllll.github.io/bitsandbytes-windows-webui\n"]},{"cell_type":"markdown","metadata":{},"source":["**To resolve some issues while installing packages**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T16:47:42.428523Z","iopub.status.busy":"2024-01-27T16:47:42.427733Z","iopub.status.idle":"2024-01-27T16:47:42.434850Z","shell.execute_reply":"2024-01-27T16:47:42.433821Z","shell.execute_reply.started":"2024-01-27T16:47:42.428486Z"},"trusted":true},"outputs":[],"source":["# import pkg_resources\n","\n","# def resolve_conflicts():\n","#     \"\"\"\n","#     Resolves dependency conflicts by creating a new environment and installing compatible versions.\n","#     \"\"\"\n","\n","#     # Create a new virtual environment (replace \"myvenv\" with your desired name)\n","#     !virtualenv myvenv\n","#     !source myvenv/bin/activate\n","\n","#     # Install compatible versions of conflicting packages\n","#     required_versions = {\n","#         \"cupy-cuda11x\": \">=12.0.0\",\n","#         \"dill\": \"<0.3.2,>=0.3.1.1\",\n","#         \"numpy\": \"<1.25.0,>=1.14.3\",\n","#         \"pyarrow\": \"<10.0.0,>=3.0.0\",\n","#         \"jupyter-server\": \"~=1.16\",\n","#         \"jupyterlab\": \"~=3.4\",\n","#         \"urllib3\": \"<2.1,>=1.25.4\",  # Ensure python_version >= \"3.10\"\n","#         \"pandas\": \"<1.6.0dev0,>=1.3\",\n","#         \"protobuf\": \"<5,>=4.21\",\n","#         \"dask\": \"==2023.7.1\",\n","#         \"distributed\": \"==2023.7.1\",\n","#         \"fsspec\": \"==2023.6.0\",\n","#         \"urllib3\": \"<2.0\",\n","#         \"google-api-core[grpc]\": \"<2.0.0dev,>=1.22.2\",\n","#         \"packaging\": \"<22.0dev,>=14.3\",\n","#         \"grpcio\": \"<2.0dev,>=1.51.3\",\n","#         \"jupyter-lsp\": \">=2.0.0\",\n","#         \"google-cloud-storage\": \"<3,>=2.2.1\",\n","#         \"shapely\": \">=2.0.1\",\n","#         \"numpy\": \"<1.25,>=1.21\",\n","#         \"cryptography\": \"<42,>=38.0.0\",\n","#         \"numpy\": \"<1.22.2,>=1.15.0\",\n","#         \"scipy\": \"<1.8.0,>=1.7.3\",\n","#         \"fsspec\": \"==2023.12.2\",\n","#         \"typing-extensions\": \"<4.6.0,>=3.6.6\",\n","#         \"platformdirs\": \"<4,>=2.4\",\n","#         \"numpy\": \"<1.24,>=1.16.0\",\n","#         \"pandas\": \"!=1.4.0,<2.1,>1.1\",\n","#         \"scipy\": \"<1.12,>=1.4.1\",\n","#     }\n","\n","#     for package, version_spec in required_versions.items():\n","#         !pip install \"{package} {version_spec}\"\n","\n","#     # Verify that conflicts are resolved\n","#     working_set = pkg_resources.WorkingSet()\n","#     if not working_set.require(required_versions):\n","#         raise RuntimeError(\"Dependency conflicts could not be resolved.\")\n","\n","#     print(\"Dependency conflicts resolved!\")\n","\n","# if __name__ == \"__main__\":\n","#     resolve_conflicts()\n"]},{"cell_type":"markdown","metadata":{},"source":["**Importing necessary packages**"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T16:47:42.436484Z","iopub.status.busy":"2024-01-27T16:47:42.436128Z","iopub.status.idle":"2024-01-27T16:47:43.627715Z","shell.execute_reply":"2024-01-27T16:47:43.625715Z","shell.execute_reply.started":"2024-01-27T16:47:42.436450Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import os\n","import torch\n","import transformers\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","from peft import prepare_model_for_kbit_training\n","from peft import LoraConfig, get_peft_model\n","from trl import SFTTrainer"]},{"cell_type":"markdown","metadata":{},"source":["**Model: https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.3?text=My+name+is+Clara+and+I+am**"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.629729Z","iopub.status.idle":"2024-01-27T16:47:43.630082Z","shell.execute_reply":"2024-01-27T16:47:43.629937Z","shell.execute_reply.started":"2024-01-27T16:47:43.629921Z"},"trusted":true},"outputs":[],"source":["# Model from Hugging Face hub\n","base_model = \"PY007/TinyLlama-1.1B-Chat-v0.3\"\n","\n","# Fine-tuned model\n","new_model = \"TinyLlama-chat-medaid-model\""]},{"cell_type":"markdown","metadata":{},"source":["**Reading the PDF and storing it as a text file**"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.631566Z","iopub.status.idle":"2024-01-27T16:47:43.631903Z","shell.execute_reply":"2024-01-27T16:47:43.631730Z","shell.execute_reply.started":"2024-01-27T16:47:43.631716Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Generating train split: 53861 examples [00:00, 1282424.91 examples/s]\n"]}],"source":["# READING THE NEW CONVERSATION DATASET WITH ALL THE TEXT FILES (doctor-patient-conversation-large)\n","\n","# Directory containing your text files\n","text_files_directory = r\"G:\\LLM-Model-MedAid-Thesis\\Model-3\\Dataset\"\n","\n","# List to store individual conversation texts\n","conversation_texts = []\n","\n","# Loop through each text file in the directory\n","for filename in os.listdir(text_files_directory):\n","    if filename.endswith(\".txt\"):\n","        file_path = os.path.join(text_files_directory, filename)\n","        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as file:\n","            # Use errors=\"ignore\" to skip characters that cannot be decoded\n","            conversation_texts.append(file.read())\n","\n","# Save the combined text to a file\n","output_path = r\"G:\\LLM-Model-MedAid-Thesis\\Model-3\\combined_conversations.txt\"\n","with open(output_path, \"w\", encoding=\"utf-8\") as output_file:\n","    for text in conversation_texts:\n","        output_file.write(text + \"\\n\\n\")\n","\n","dataset = load_dataset(\"text\", data_files=output_path, split=\"train\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.632802Z","iopub.status.idle":"2024-01-27T16:47:43.633132Z","shell.execute_reply":"2024-01-27T16:47:43.632980Z","shell.execute_reply.started":"2024-01-27T16:47:43.632965Z"},"trusted":true},"outputs":[],"source":["print(dataset)"]},{"cell_type":"markdown","metadata":{},"source":["**This code has a configuration for quantization using the *BitsAndBytesConfig class* from the *trl library*. Quantization is a technique used in deep learning to reduce the memory and computational requirements of neural networks.**"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.634612Z","iopub.status.idle":"2024-01-27T16:47:43.634961Z","shell.execute_reply":"2024-01-27T16:47:43.634814Z","shell.execute_reply.started":"2024-01-27T16:47:43.634794Z"},"trusted":true},"outputs":[],"source":["compute_dtype = getattr(torch, \"float16\")\n","\n","quant_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    # bnb_4bit_compute_dtype=compute_dtype,\n","    bnb_4bit_compute_dtype=torch.bfloat16,\n","    bnb_4bit_use_double_quant=False,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","from torch.utils.cpp_extension import CUDA_HOME\n","print(CUDA_HOME) # by default it is set to /usr/local/cuda/"]},{"cell_type":"markdown","metadata":{},"source":["**This code is using the AutoModelForCausalLM class from the transformers library to instantiate a pre-trained causal language model with specific configurations.**"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.636678Z","iopub.status.idle":"2024-01-27T16:47:43.637011Z","shell.execute_reply":"2024-01-27T16:47:43.636868Z","shell.execute_reply.started":"2024-01-27T16:47:43.636853Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cuda  True\n"]}],"source":["import torch\n","print(\"Cuda \", torch.cuda.is_available())\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    base_model,\n","    quantization_config=quant_config,\n","    # device_map={\"\": 0}\n","    device_map=\"auto\", # automatically figures out how to best use CPU + GPU for loading model\n","    trust_remote_code=True, # prevents running custom model files on your machine\n",")\n","\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1\n"]},{"cell_type":"markdown","metadata":{},"source":["**This code is using the AutoTokenizer class from the transformers library to instantiate a tokenizer for a pre-trained language model.**"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.638414Z","iopub.status.idle":"2024-01-27T16:47:43.638745Z","shell.execute_reply":"2024-01-27T16:47:43.638600Z","shell.execute_reply.started":"2024-01-27T16:47:43.638584Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["\n","# Create the tokenizer on the chosen device\n","# tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n","tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\""]},{"cell_type":"markdown","metadata":{},"source":["Prepare Model for Training"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["model.train() # model in training mode (dropout modules are activated)\n","\n","# enable gradient check pointing\n","model.gradient_checkpointing_enable()\n","\n","# enable quantized training\n","model = prepare_model_for_kbit_training(model)"]},{"cell_type":"markdown","metadata":{},"source":["**This code is defining a configuration for the LoRA (Local Reparameterization with Attention) model using the LoraConfig class, which appears to be part of the peft library.**"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.639681Z","iopub.status.idle":"2024-01-27T16:47:43.640010Z","shell.execute_reply":"2024-01-27T16:47:43.639868Z","shell.execute_reply.started":"2024-01-27T16:47:43.639853Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable params: 720,896 || all params: 1,100,781,568 || trainable%: 0.0654894686608706\n"]}],"source":["# Create a LoraConfig instance\n","peft_params = LoraConfig(\n","    r=8,\n","    lora_alpha=32,\n","    target_modules=[\"q_proj\"],\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","# LoRA trainable version of model\n","model = get_peft_model(model, peft_params)\n","\n","# trainable parameter count\n","model.print_trainable_parameters()"]},{"cell_type":"markdown","metadata":{},"source":["**Preparing Training Dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # create tokenize function\n","# def tokenize_function(examples):\n","#     # extract text\n","#     text = examples[\"example\"]\n","\n","#     #tokenize and truncate text\n","#     tokenizer.truncation_side = \"left\"\n","#     tokenized_inputs = tokenizer(\n","#         text,\n","#         return_tensors=\"np\",\n","#         truncation=True,\n","#         max_length=512\n","#     )\n","\n","#     return tokenized_inputs\n","\n","# # tokenize training and validation datasets\n","# tokenized_data = dataset.map(tokenize_function, batched=True)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# setting pad token\n","tokenizer.pad_token = tokenizer.eos_token\n","# data collator\n","data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)"]},{"cell_type":"markdown","metadata":{},"source":["**This code is defining a set of training parameters using the TrainingArguments class, which is often used in the transformers library for configuring training settings.**"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.641661Z","iopub.status.idle":"2024-01-27T16:47:43.642104Z","shell.execute_reply":"2024-01-27T16:47:43.641905Z","shell.execute_reply.started":"2024-01-27T16:47:43.641884Z"},"trusted":true},"outputs":[],"source":["# hyperparameters\n","lr = 2e-4\n","batch_size = 4\n","num_epochs = 10\n","\n","# Specify the output directory and other training parameters\n","training_params = TrainingArguments(\n","    output_dir=\"./results\",\n","    learning_rate=lr,\n","    per_device_train_batch_size=batch_size,\n","    num_train_epochs=num_epochs,\n","    gradient_accumulation_steps=4,\n","    # save_steps=25,\n","    logging_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    # logging_steps=25,  \n","    weight_decay=0.01,\n","    # load_best_model_at_end=True,\n","    warmup_steps=2,\n","    fp16=True,\n","    optim=\"paged_adamw_8bit\",\n","    # bf16=False,\n","    # max_grad_norm=0.3,\n","    # max_steps=-1,\n","    # warmup_ratio=0.03,\n","    # group_by_length=True,\n","    # lr_scheduler_type=\"constant\",\n","    # report_to=\"tensorboard\"\n",")"]},{"cell_type":"markdown","metadata":{},"source":["**This code is creating an instance of the SFTTrainer class, presumably from the trl library, to facilitate the training of a model using the specified configuration.**\n","\n","**SFTTrainer instance is configured with the** \n","* model, \n","* training dataset, \n","* Peft configuration, \n","* tokenizer,\n","* training arguments. \n","\n","**The specific behavior and training process are determined by the SFTTrainer implementation in the trl library, and the configured parameters influence aspects such as optimization, learning rate, and model architecture during training.**"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.643115Z","iopub.status.idle":"2024-01-27T16:47:43.643436Z","shell.execute_reply":"2024-01-27T16:47:43.643292Z","shell.execute_reply.started":"2024-01-27T16:47:43.643277Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\User\\AppData\\Roaming\\Python\\Python310\\site-packages\\trl\\trainer\\sft_trainer.py:222: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n","  warnings.warn(\n","Map: 100%|██████████| 53861/53861 [00:00<00:00, 89470.38 examples/s]\n"]}],"source":["# Create the trainer\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_params,\n","    dataset_text_field=\"text\",\n","    max_seq_length=None,\n","    tokenizer=tokenizer,\n","    args=training_params,\n","    packing=False,\n","    data_collator=data_collator\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.644647Z","iopub.status.idle":"2024-01-27T16:47:43.644991Z","shell.execute_reply":"2024-01-27T16:47:43.644847Z","shell.execute_reply.started":"2024-01-27T16:47:43.644832Z"},"trusted":true},"outputs":[],"source":["# train model\n","model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n","# trainer.train()\n","\n","# renable warnings\n","model.config.use_cache = True"]},{"cell_type":"markdown","metadata":{},"source":["**The code you provided is saving the trained model and its associated tokenizer to a specified directory using the save_pretrained method.**"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["model_path = \"preTrained_model\""]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.646365Z","iopub.status.idle":"2024-01-27T16:47:43.646701Z","shell.execute_reply":"2024-01-27T16:47:43.646547Z","shell.execute_reply.started":"2024-01-27T16:47:43.646531Z"},"trusted":true},"outputs":[{"data":{"text/plain":["('preTrained_model\\\\tokenizer_config.json',\n"," 'preTrained_model\\\\special_tokens_map.json',\n"," 'preTrained_model\\\\tokenizer.model',\n"," 'preTrained_model\\\\added_tokens.json',\n"," 'preTrained_model\\\\tokenizer.json')"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["trainer.save_model(model_path)\n","tokenizer.save_pretrained(model_path)\n","\n","# trainer.model.save_pretrained(new_model)\n","# trainer.tokenizer.save_pretrained(new_model)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["# Import the pretrained model\n","model = AutoModelForCausalLM.from_pretrained(model_path)\n","tokenizer = AutoTokenizer.from_pretrained(model_path)"]},{"cell_type":"markdown","metadata":{},"source":["**This code creates a simple conversational loop that simulates a doctor assistant interaction. It uses a fine-tuned language model (assumed to be a text generation model) and a tokenizer to generate responses based on user input. The conversation is logged in a text file, and the loop continues until the user provides an exit signal.**"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.647706Z","iopub.status.idle":"2024-01-27T16:47:43.648052Z","shell.execute_reply":"2024-01-27T16:47:43.647908Z","shell.execute_reply.started":"2024-01-27T16:47:43.647892Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Both `max_new_tokens` (=32) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"name":"stdout","output_type":"stream","text":["<s>[Prompt] Be a doctor assistant. And keep questioning one by one to extract symptoms and history of the patient. Don't give advice or ask anything else. Just extract symptoms or history by questioning one question at a time. \n"," [User response] \n"," I have chest pain. \n"," [/User Response] \n","\n","[Prompt] Now, ask me about my symptoms. \n"," [User response] \n"," I have chest pain and dizz\n"]},{"name":"stderr","output_type":"stream","text":["Both `max_new_tokens` (=32) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]},{"name":"stdout","output_type":"stream","text":["<s>[Prompt] Be a doctor assistant. And keep questioning one by one to extract symptoms and history of the patient. Don't give advice or ask anything else. Just extract symptoms or history by questioning one question at a time. \n"," [User response] \n"," I have pain in my chest  \n"," [/User Response] \n","\n"," 2. Question [Prompt] Be a doctor assistant. And keep questioning one by one to extract symptoms and history of the patient\n","Ending the conversation.\n"]}],"source":["# logging.set_verbosity(logging.CRITICAL)\n","\n","# Initial prompt\n","prompt = \"Be a doctor assistant. And keep questioning one by one to extract symptoms and history of the patient. Don't give advice or ask anything else. Just extract symptoms or history by questioning one question at a time.\"\n","\n","# Create a pipeline for text generation using the fine-tuned model and tokenizer\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n","\n","# File to save the conversation\n","output_file_path = \"G:\\LLM-Model-MedAid-Thesis\\Model-3\\conversation_log.txt\"  # Update 'your_folder' with the desired folder in your Drive\n","\n","# Function to ask a question and get the user's response\n","def ask_question_and_log(prompt, user_response, file_path):\n","    # Ask the question\n","    result = pipe(f\"<s>[Prompt] {prompt} \\n [User response] \\n {user_response} \\n [/User Response]\")\n","\n","    # Get the generated text (question)\n","    generated_text = result[0]['generated_text']\n","\n","    # Print and save the generated text (question)\n","    print(generated_text)\n","    with open(file_path, \"a\", encoding=\"utf-8\") as output_file:\n","        output_file.write(f\"[Model] {generated_text}\\n\\n\")\n","\n","    # Simulate user answering the question\n","    user_response = input(\"Your response: \")  # User provides input\n","\n","    return user_response\n","\n","# Initial user response\n","user_response = \"I have chest pain.\"\n","\n","# Ask a question based on the user's response and log the conversation\n","with open(output_file_path, \"a\", encoding=\"utf-8\") as output_file:\n","    output_file.write(\"\\nConversation started.\\n\\n\")\n","\n","# Loop to continue the conversation\n","while True:\n","    user_response = ask_question_and_log(prompt, user_response, output_file_path)\n","\n","    # Check for an exit condition (e.g., user response indicating the end of the conversation)\n","    if \"exit\" in user_response.lower():\n","        print(\"Ending the conversation.\")\n","\n","        # Save the conversation to a file\n","        with open(output_file_path, \"a\", encoding=\"utf-8\") as output_file:\n","            output_file.write(\"\\nConversation ended by user. ---------------- \\n\\n\")\n","\n","        break\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["As a doctor assistant, I need to understand why you've come today. Please describe your symptoms or reasons for the visit.\n","<s>[Prompt] Keep questioning to extract symptoms or history: \n"," [User response] \n"," I have chest pain \n"," [/User Response] \n","\n","[Prompt] Keep questioning to extract symptoms or history: \n"," [User response] \n"," I have chest pain and short\n","<s>[Prompt] Keep questioning to extract symptoms or history: \n"," [User response] \n"," I have allergies too \n"," [/User Response] \n","\n","[Prompt] What are your symptoms? [/Prompt] \n","\n","[User Response] \n"," I have a runny\n","<s>[Prompt] Keep questioning to extract symptoms or history: \n"," [User response] \n"," I so I've just had this pain in my chest for just over a week now and it's caused me to have trouble breathing. \n"," [/User Response]\n","\n","I'm not sure if this is the best way to phrase this, but I'm trying to extract information from the user about their chest\n","<s>[Prompt] Keep questioning to extract symptoms or history: \n"," [User response] \n"," if I take a break or sit down to rest then I can usually catch my breath. But if even if I'm doing a little bit of exertion like walking a few a few blocks or something like that I'm finding it, I'm I'm short of breath. \n"," [/User Response]\n","\n","\n","I'm not sure if this is the best way to phrase the question, but I'm trying to get a sense of the patient'\n","<s>[Prompt] Keep questioning to extract symptoms or history: \n"," [User response] \n"," no wheezing. \n"," [/User Response]\n","\n","I want to extract the following symptoms from the above prompt:\n","\n","*\n","\n","*Wheezing\n","\n","*No wheezing\n","<s>[Prompt] Keep questioning to extract symptoms or history: \n"," [User response] \n"," Hearing has been OK. \n"," [/User Response]\n","\n","I want to extract the following information from the prompt:\n","\n","  - Hearing has been OK.\n","\n","I tried to use the following\n","Ending the conversation.\n"]}],"source":["# Create a pipeline for text generation\n","pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n","\n","# File to save the conversation\n","output_file_path = \"G:\\LLM-Model-MedAid-Thesis\\Model-3\\conversation_log.txt\"  # Update 'your_folder' with the desired folder in your Drive\n","\n","# Function to start the conversation and log it\n","def start_conversation_and_log(file_path):\n","    # Start conversation by asking why the user has come today\n","    initial_prompt = \"As a doctor assistant, I need to understand why you've come today. Please describe your symptoms or reasons for the visit.\"\n","    print(initial_prompt)\n","    with open(file_path, \"a\", encoding=\"utf-8\") as output_file:\n","        output_file.write(f\"[System] {initial_prompt}\\n\\n\")\n","\n","# Function to ask a question and get the user's response\n","def ask_question_and_log(prompt, user_response, file_path):\n","    # Ask the question\n","    result = pipe(f\"<s>[Prompt] {prompt} \\n [User response] \\n {user_response} \\n [/User Response]\")\n","\n","    # Get the generated text (question)\n","    generated_text = result[0]['generated_text']\n","\n","    # Print and save the generated text (question)\n","    print(generated_text)\n","    with open(file_path, \"a\", encoding=\"utf-8\") as output_file:\n","        output_file.write(f\"[Model] {generated_text}\\n\\n\")\n","\n","    # Simulate user answering the question\n","    user_response = input(\"Your response: \")  # User provides input\n","\n","    return user_response\n","\n","# Start the conversation\n","start_conversation_and_log(output_file_path)\n","\n","# Ask questions based on the user's responses and log the conversation\n","with open(output_file_path, \"a\", encoding=\"utf-8\") as output_file:\n","    output_file.write(\"\\nConversation started.\\n\\n\")\n","\n","# Initial user response\n","user_response = input(\"Your response: \")  # User provides input\n","\n","# Loop to continue the conversation\n","while True:\n","    user_response = ask_question_and_log(\"Keep questioning to extract symptoms or history:\", user_response, output_file_path)\n","\n","    # Check for an exit condition (e.g., user response indicating the end of the conversation)\n","    if \"exit\" in user_response.lower():\n","        print(\"Ending the conversation.\")\n","\n","        # Save the conversation to a file\n","        with open(output_file_path, \"a\", encoding=\"utf-8\") as output_file:\n","            output_file.write(\"\\nConversation ended by user. ---------------- \\n\\n\")\n","\n","        break\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4256693,"sourceId":7332627,"sourceType":"datasetVersion"},{"datasetId":4354465,"sourceId":7480410,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"tinyLlamaMedAid","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.1"}},"nbformat":4,"nbformat_minor":4}
