{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-16T12:10:36.033473Z","iopub.status.busy":"2024-04-16T12:10:36.032823Z","iopub.status.idle":"2024-04-16T12:10:36.824186Z","shell.execute_reply":"2024-04-16T12:10:36.823214Z","shell.execute_reply.started":"2024-04-16T12:10:36.033441Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T12:10:36.826284Z","iopub.status.busy":"2024-04-16T12:10:36.825916Z","iopub.status.idle":"2024-04-16T12:10:55.588448Z","shell.execute_reply":"2024-04-16T12:10:55.587173Z","shell.execute_reply.started":"2024-04-16T12:10:36.826259Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (4.36.2)\n","Requirement already satisfied: langchain in c:\\python310\\lib\\site-packages (0.1.16)\n","Requirement already satisfied: filelock in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.20.2)\n","Requirement already satisfied: numpy>=1.17 in c:\\python310\\lib\\site-packages (from transformers) (1.24.2)\n","Requirement already satisfied: packaging>=20.0 in c:\\python310\\lib\\site-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2023.8.8)\n","Requirement already satisfied: requests in c:\\python310\\lib\\site-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in c:\\python310\\lib\\site-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in c:\\python310\\lib\\site-packages (from transformers) (4.66.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\python310\\lib\\site-packages (from langchain) (2.0.29)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from langchain) (3.9.1)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from langchain) (4.0.3)\n","Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\python310\\lib\\site-packages (from langchain) (0.6.4)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\python310\\lib\\site-packages (from langchain) (1.33)\n","Requirement already satisfied: langchain-community<0.1,>=0.0.32 in c:\\python310\\lib\\site-packages (from langchain) (0.0.33)\n","Requirement already satisfied: langchain-core<0.2.0,>=0.1.42 in c:\\python310\\lib\\site-packages (from langchain) (0.1.43)\n","Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in c:\\python310\\lib\\site-packages (from langchain) (0.0.1)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\python310\\lib\\site-packages (from langchain) (0.1.48)\n","Requirement already satisfied: pydantic<3,>=1 in c:\\python310\\lib\\site-packages (from langchain) (2.7.0)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\python310\\lib\\site-packages (from langchain) (8.2.3)\n","Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n","Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\python310\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\python310\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n","Requirement already satisfied: jsonpointer>=1.9 in c:\\python310\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\python310\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.1)\n","Requirement already satisfied: annotated-types>=0.4.0 in c:\\python310\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.18.1 in c:\\python310\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.18.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests->transformers) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\python310\\lib\\site-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: greenlet!=0.4.17 in c:\\python310\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n","Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\python310\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip is available: 23.3.2 -> 24.0\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]}],"source":["%pip install transformers langchain\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: requests==2.27.1 in c:\\python310\\lib\\site-packages (2.27.1)Note: you may need to restart the kernel to use updated packages.\n","\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests==2.27.1) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests==2.27.1) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\python310\\lib\\site-packages (from requests==2.27.1) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests==2.27.1) (3.4)\n"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip is available: 23.3.2 -> 24.0\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]}],"source":["%pip install requests==2.27.1   "]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T12:10:55.590138Z","iopub.status.busy":"2024-04-16T12:10:55.589853Z","iopub.status.idle":"2024-04-16T12:10:55.594982Z","shell.execute_reply":"2024-04-16T12:10:55.593935Z","shell.execute_reply.started":"2024-04-16T12:10:55.590111Z"},"trusted":true},"outputs":[],"source":["import os\n","\n","os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n","# os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"G:\\hugging_face_hub_cache\"\n","os.environ['CURL_CA_BUNDLE'] = ''"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T12:10:55.597341Z","iopub.status.busy":"2024-04-16T12:10:55.597089Z","iopub.status.idle":"2024-04-16T12:12:14.845723Z","shell.execute_reply":"2024-04-16T12:12:14.844594Z","shell.execute_reply.started":"2024-04-16T12:10:55.597319Z"},"trusted":true},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'huggingface_hub'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn [8], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConversationChain\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2LMHeadModel, GPT2Tokenizer, pipeline\n\u001b[0;32m      7\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjianghc/medical_chatbot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m saved_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedical_chatbot_model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\__init__.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m     29\u001b[0m     _LazyModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m     logging,\n\u001b[0;32m     47\u001b[0m )\n\u001b[0;32m     50\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\dependency_versions_check.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[0;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m ]\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\utils\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/env python\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# coding=utf-8\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_full_repo_name  \u001b[38;5;66;03m# for backward compatibility\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'huggingface_hub'"]}],"source":["from langchain.llms import HuggingFacePipeline\n","from langchain.chains import ConversationChain\n","import torch\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n","\n","\n","path = \"jianghc/medical_chatbot\"\n","saved_model_path = \"medical_chatbot_model\"\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# tokenizer = GPT2Tokenizer.from_pretrained(path, max_new_tokens=1024, cache_dir=\"G:\\hugging_face_hub_cache\")\n","# model = GPT2LMHeadModel.from_pretrained(path, cache_dir=\"G:\\hugging_face_hub_cache\").to(device)\n","\n","tokenizer = GPT2Tokenizer.from_pretrained(saved_model_path, max_new_tokens=1024)\n","model = GPT2LMHeadModel.from_pretrained(saved_model_path).to(device)\n","\n","pipe = pipeline(\n","    \"text-generation\", model=model, tokenizer=tokenizer, max_length=1024\n",")\n","model.sav\n","\n","llm = HuggingFacePipeline(pipeline=pipe)\n","\n","conversation = ConversationChain(llm=llm)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.save_model(saved_model_path)\n","tokenizer.save_pretrained(saved_model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T12:12:14.847738Z","iopub.status.busy":"2024-04-16T12:12:14.847097Z","iopub.status.idle":"2024-04-16T12:12:14.853030Z","shell.execute_reply":"2024-04-16T12:12:14.852161Z","shell.execute_reply.started":"2024-04-16T12:12:14.847708Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","{history}\n","Human: {input}\n","AI:\n"]}],"source":["print(conversation.prompt.template)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T12:12:14.854360Z","iopub.status.busy":"2024-04-16T12:12:14.854095Z","iopub.status.idle":"2024-04-16T12:12:20.684323Z","shell.execute_reply":"2024-04-16T12:12:20.683361Z","shell.execute_reply.started":"2024-04-16T12:12:14.854337Z"},"trusted":true},"outputs":[],"source":["from langchain.chains.conversation.memory import ConversationBufferMemory\n","\n","conversation_buf = ConversationChain(\n","    llm=llm,\n","    memory=ConversationBufferMemory()\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-16T12:12:20.686192Z","iopub.status.busy":"2024-04-16T12:12:20.685627Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n","  warn_deprecated(\n"]}],"source":["conversation_buf(\"Good morning AI!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from langchain.callbacks import get_openai_callback\n","\n","def count_tokens(chain, query):\n","    with get_openai_callback() as cb:\n","        result = chain.run(query)\n","        print(f'Spent a total of {cb.total_tokens} tokens')\n","\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["count_tokens(\n","    conversation_buf, \n","    \"My interest here is to explore the potential of integrating Large Language Models with external knowledge\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["count_tokens(\n","    conversation_buf,\n","    \"I just want to analyze the different possibilities. What can you think of?\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["count_tokens(\n","    conversation_buf, \n","    \"Which data source types could be used to give context to the model?\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["count_tokens(\n","    conversation_buf, \n","    \"What is my aim again?\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(conversation_buf.memory.buffer)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from langchain.chains.conversation.memory import ConversationSummaryMemory\n","\n","conversation = ConversationChain(\n","\tllm=llm,\n","\tmemory=ConversationSummaryMemory(llm=llm)\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(conversation_sum.memory.prompt.template)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# without count_tokens we'd call `conversation_sum(\"Good morning AI!\")`\n","# but let's keep track of our tokens:\n","count_tokens(\n","    conversation_sum, \n","    \"Good morning AI!\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["count_tokens(\n","    conversation_sum, \n","    \"My interest here is to explore the potential of integrating Large Language Models with external knowledge\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["count_tokens(\n","    conversation_sum, \n","    \"I just want to analyze the different possibilities. What can you think of?\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["count_tokens(\n","    conversation_sum, \n","    \"Which data source types could be used to give context to the model?\"\n",")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30683,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.1"}},"nbformat":4,"nbformat_minor":4}
