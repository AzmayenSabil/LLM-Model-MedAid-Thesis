{"cells":[{"cell_type":"markdown","metadata":{},"source":["INSTALL PACKAGES"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T16:47:12.027287Z","iopub.status.busy":"2024-01-27T16:47:12.026981Z","iopub.status.idle":"2024-01-27T16:47:42.426119Z","shell.execute_reply":"2024-01-27T16:47:42.425125Z","shell.execute_reply.started":"2024-01-27T16:47:12.027264Z"},"trusted":true},"outputs":[],"source":["# # You only need to run this once per machine\n","# %pip install -q -U bitsandbytes\n","# %pip install -q -U git+https://github.com/huggingface/transformers.git\n","# %pip install -q -U git+https://github.com/huggingface/peft.git\n","# %pip install -q -U git+https://github.com/huggingface/accelerate.git\n","# %pip install -q -U datasets scipy ipywidgets matplotlib einops\n"]},{"cell_type":"markdown","metadata":{},"source":["**Accelerator**"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T16:47:42.428523Z","iopub.status.busy":"2024-01-27T16:47:42.427733Z","iopub.status.idle":"2024-01-27T16:47:42.434850Z","shell.execute_reply":"2024-01-27T16:47:42.433821Z","shell.execute_reply.started":"2024-01-27T16:47:42.428486Z"},"trusted":true},"outputs":[],"source":["from accelerate import FullyShardedDataParallelPlugin, Accelerator\n","from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n","\n","fsdp_plugin = FullyShardedDataParallelPlugin(\n","    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n","    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",")\n","\n","accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"]},{"cell_type":"markdown","metadata":{},"source":["**Weights & Biases**"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-01-27T16:47:42.436484Z","iopub.status.busy":"2024-01-27T16:47:42.436128Z","iopub.status.idle":"2024-01-27T16:47:43.627715Z","shell.execute_reply":"2024-01-27T16:47:43.625715Z","shell.execute_reply.started":"2024-01-27T16:47:42.436450Z"},"trusted":true},"outputs":[],"source":["# %pip install -q wandb -U\n","\n","# import wandb, os\n","# wandb.login()\n","\n","# wandb_project = \"journal-finetune\"\n","# if len(wandb_project) > 0:\n","#     os.environ[\"WANDB_PROJECT\"] = wandb_project"]},{"cell_type":"markdown","metadata":{},"source":["**Load Dataset**"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.631566Z","iopub.status.idle":"2024-01-27T16:47:43.631903Z","shell.execute_reply":"2024-01-27T16:47:43.631730Z","shell.execute_reply.started":"2024-01-27T16:47:43.631716Z"},"trusted":true},"outputs":[],"source":["from datasets import load_dataset\n","\n","train_dataset = load_dataset('json', data_files='train_conversation_data.jsonl', split='train')\n","eval_dataset = load_dataset('json', data_files='test_conversation_data.jsonl', split='train')"]},{"cell_type":"markdown","metadata":{},"source":["**Formatting prompts**"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.632802Z","iopub.status.idle":"2024-01-27T16:47:43.633132Z","shell.execute_reply":"2024-01-27T16:47:43.632980Z","shell.execute_reply.started":"2024-01-27T16:47:43.632965Z"},"trusted":true},"outputs":[],"source":["def formatting_func(example):\n","    text = f\"### The following is a note by Eevee the Dog: {example['conversation_pairs']}\"\n","    return text"]},{"cell_type":"markdown","metadata":{},"source":["**Load Base Model**"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.634612Z","iopub.status.idle":"2024-01-27T16:47:43.634961Z","shell.execute_reply":"2024-01-27T16:47:43.634814Z","shell.execute_reply.started":"2024-01-27T16:47:43.634794Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.39s/it]\n"]}],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","base_model_id = \"microsoft/phi-2\"\n","model = AutoModelForCausalLM.from_pretrained(base_model_id, trust_remote_code=True, torch_dtype=torch.float16, load_in_8bit=True)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.4\n"]}],"source":["from torch.utils.cpp_extension import CUDA_HOME\n","print(CUDA_HOME) # by default it is set to /usr/local/cuda/"]},{"cell_type":"markdown","metadata":{},"source":["**Tokenization**"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.636678Z","iopub.status.idle":"2024-01-27T16:47:43.637011Z","shell.execute_reply":"2024-01-27T16:47:43.636868Z","shell.execute_reply.started":"2024-01-27T16:47:43.636853Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(\n","    base_model_id,\n","    padding_side=\"left\",\n","    add_eos_token=True,\n","    add_bos_token=True,\n","    use_fast=False, # needed for now, should be fixed soon\n",")\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","def generate_and_tokenize_prompt(prompt):\n","    return tokenizer(formatting_func(prompt))\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Map:   0%|          | 0/37 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2153 > 2048). Running this sequence through the model will result in indexing errors\n","Map: 100%|██████████| 37/37 [00:00<00:00, 109.14 examples/s]\n"]}],"source":["tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n","tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting matplotlib\n","  Using cached matplotlib-3.8.3-cp310-cp310-win_amd64.whl.metadata (5.9 kB)\n","Collecting contourpy>=1.0.1 (from matplotlib)\n","  Using cached contourpy-1.2.0-cp310-cp310-win_amd64.whl.metadata (5.8 kB)\n","Collecting cycler>=0.10 (from matplotlib)\n","  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n","Requirement already satisfied: fonttools>=4.22.0 in c:\\python310\\lib\\site-packages (from matplotlib) (4.50.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in c:\\python310\\lib\\site-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: numpy<2,>=1.21 in c:\\python310\\lib\\site-packages (from matplotlib) (1.24.2)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (21.3)\n","Requirement already satisfied: pillow>=8 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (10.2.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Using cached matplotlib-3.8.3-cp310-cp310-win_amd64.whl (7.6 MB)\n","Using cached contourpy-1.2.0-cp310-cp310-win_amd64.whl (186 kB)\n","Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n","Installing collected packages: cycler, contourpy, matplotlib\n","Successfully installed contourpy-1.2.0 cycler-0.12.1 matplotlib-3.8.3\n","Note: you may need to restart the kernel to use updated packages.\n"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip is available: 23.3.2 -> 24.0\n","[notice] To update, run: python.exe -m pip install --upgrade pip\n"]}],"source":["%pip install matplotlib"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# import matplotlib.pyplot as plt\n","\n","# def plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):\n","#     lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n","#     lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n","#     print(len(lengths))\n","\n","#     # Plotting the histogram\n","#     plt.figure(figsize=(10, 6))\n","#     plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n","#     plt.xlabel('Length of input_ids')\n","#     plt.ylabel('Frequency')\n","#     plt.title('Distribution of Lengths of input_ids')\n","#     plt.show()\n","\n","# plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["max_length = 512 # This was an appropriate max length for my dataset\n","\n","def generate_and_tokenize_prompt2(prompt):\n","    result = tokenizer(\n","        formatting_func(prompt),\n","        truncation=True,\n","        max_length=max_length,\n","        padding=\"max_length\",\n","    )\n","    result[\"labels\"] = result[\"input_ids\"].copy()\n","    return result"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt2)\n","tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt2)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 21017, 383, 1708, 318, 257, 3465, 416, 412, 44655, 68, 262, 8532, 25, 17635]\n"]}],"source":["print(tokenized_train_dataset[1]['input_ids'])"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"]},{"cell_type":"markdown","metadata":{},"source":["**Set Up LoRA**"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.638414Z","iopub.status.idle":"2024-01-27T16:47:43.638745Z","shell.execute_reply":"2024-01-27T16:47:43.638600Z","shell.execute_reply.started":"2024-01-27T16:47:43.638584Z"},"trusted":true},"outputs":[],"source":["def print_trainable_parameters(model):\n","    \"\"\"\n","    Prints the number of trainable parameters in the model.\n","    \"\"\"\n","    trainable_params = 0\n","    all_param = 0\n","    for _, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(\n","        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n","    )"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["PhiForCausalLM(\n","  (model): PhiModel(\n","    (embed_tokens): Embedding(51200, 2560)\n","    (embed_dropout): Dropout(p=0.0, inplace=False)\n","    (layers): ModuleList(\n","      (0-31): 32 x PhiDecoderLayer(\n","        (self_attn): PhiAttention(\n","          (q_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n","          (k_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n","          (v_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n","          (dense): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n","          (rotary_emb): PhiRotaryEmbedding()\n","        )\n","        (mlp): PhiMLP(\n","          (activation_fn): NewGELUActivation()\n","          (fc1): Linear8bitLt(in_features=2560, out_features=10240, bias=True)\n","          (fc2): Linear8bitLt(in_features=10240, out_features=2560, bias=True)\n","        )\n","        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n","        (resid_dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",")\n"]}],"source":["print(model)"]},{"cell_type":"markdown","metadata":{},"source":[" LoRA config"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable params: 26214400 || all params: 2805898240 || trainable%: 0.9342605382581515\n"]}],"source":["from peft import LoraConfig, get_peft_model\n","\n","config = LoraConfig(\n","    r=32,\n","    lora_alpha=64,\n","    target_modules=[\n","        \"Wqkv\",\n","        \"fc1\",\n","        \"fc2\",\n","    ],\n","    bias=\"none\",\n","    lora_dropout=0.05,  # Conventional\n","    task_type=\"CAUSAL_LM\",\n",")\n","\n","model = get_peft_model(model, config)\n","print_trainable_parameters(model)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["PeftModelForCausalLM(\n","  (base_model): LoraModel(\n","    (model): PhiForCausalLM(\n","      (model): PhiModel(\n","        (embed_tokens): Embedding(51200, 2560)\n","        (embed_dropout): Dropout(p=0.0, inplace=False)\n","        (layers): ModuleList(\n","          (0-31): 32 x PhiDecoderLayer(\n","            (self_attn): PhiAttention(\n","              (q_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n","              (k_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n","              (v_proj): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n","              (dense): Linear8bitLt(in_features=2560, out_features=2560, bias=True)\n","              (rotary_emb): PhiRotaryEmbedding()\n","            )\n","            (mlp): PhiMLP(\n","              (activation_fn): NewGELUActivation()\n","              (fc1): lora.Linear8bitLt(\n","                (base_layer): Linear8bitLt(in_features=2560, out_features=10240, bias=True)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=2560, out_features=32, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=32, out_features=10240, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","              (fc2): lora.Linear8bitLt(\n","                (base_layer): Linear8bitLt(in_features=10240, out_features=2560, bias=True)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=10240, out_features=32, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=32, out_features=2560, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","              )\n","            )\n","            (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n","            (resid_dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n","    )\n","  )\n",")\n"]}],"source":["print(model)"]},{"cell_type":"markdown","metadata":{},"source":["**Run Training**"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.639681Z","iopub.status.idle":"2024-01-27T16:47:43.640010Z","shell.execute_reply":"2024-01-27T16:47:43.639868Z","shell.execute_reply.started":"2024-01-27T16:47:43.639853Z"},"trusted":true},"outputs":[],"source":["model = accelerator.prepare_model(model)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["if torch.cuda.device_count() > 1: # If more than 1 GPU\n","    model.is_parallelizable = True\n","    model.model_parallel = True"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  5%|▌         | 25/500 [03:40<1:08:36,  8.67s/it]"]},{"name":"stdout","output_type":"stream","text":["{'loss': 2.0153, 'learning_rate': 2.3797595190380762e-05, 'epoch': 0.28}\n"]},{"name":"stderr","output_type":"stream","text":["                                                  \n","  5%|▌         | 25/500 [05:14<1:08:36,  8.67s/it]"]},{"name":"stdout","output_type":"stream","text":["{'eval_loss': 1.9315447807312012, 'eval_runtime': 93.5445, 'eval_samples_per_second': 0.396, 'eval_steps_per_second': 0.053, 'epoch': 0.28}\n"]}],"source":["import transformers\n","from datetime import datetime\n","\n","project = \"journal-finetune\"\n","base_model_name = \"phi2\"\n","run_name = base_model_name + \"-\" + project\n","output_dir = \"./\" + run_name\n","\n","trainer = transformers.Trainer(\n","    model=model,\n","    train_dataset=tokenized_train_dataset,\n","    eval_dataset=tokenized_val_dataset,\n","    args=transformers.TrainingArguments(\n","        output_dir=output_dir,\n","        warmup_steps=1,\n","        per_device_train_batch_size=2,\n","        gradient_accumulation_steps=1,\n","        max_steps=500,\n","        learning_rate=2.5e-5, # Want a small lr for finetuning\n","        optim=\"paged_adamw_8bit\",\n","        logging_steps=25,              # When to start reporting loss\n","        logging_dir=\"./logs\",        # Directory for storing logs\n","        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n","        save_steps=25,                # Save checkpoints every 50 steps\n","        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n","        eval_steps=25,               # Evaluate and save checkpoints every 50 steps\n","        do_eval=True,                # Perform evaluation at the end of training\n","        # report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n","        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n","    ),\n","    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",")\n","\n","model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n","trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["**Drum Roll... Try the Trained Model!**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","base_model_id = \"microsoft/phi-2\"\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    base_model_id,  # Phi2, same as before\n","    device_map=\"auto\",\n","    trust_remote_code=True,\n","    load_in_8bit=True,\n","    torch_dtype=torch.float16,\n",")\n","\n","eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True, use_fast=False)\n","eval_tokenizer.pad_token = tokenizer.eos_token"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from peft import PeftModel\n","\n","ft_model = PeftModel.from_pretrained(base_model, \"phi2-journal-finetune/checkpoint-500\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-01-27T16:47:43.641661Z","iopub.status.idle":"2024-01-27T16:47:43.642104Z","shell.execute_reply":"2024-01-27T16:47:43.641905Z","shell.execute_reply.started":"2024-01-27T16:47:43.641884Z"},"trusted":true},"outputs":[],"source":["eval_prompt = \" The following is a note by Eevee the Dog: # Today I \"\n","model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n","\n","ft_model.eval()\n","with torch.no_grad():\n","    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, repetition_penalty=1.11)[0], skip_special_tokens=True))"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4256693,"sourceId":7332627,"sourceType":"datasetVersion"},{"datasetId":4354465,"sourceId":7480410,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"tinyLlamaMedAid","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.1"}},"nbformat":4,"nbformat_minor":4}
